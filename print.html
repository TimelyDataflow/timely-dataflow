<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title></title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="introduction.html">Timely Dataflow</a></li><li class="chapter-item expanded "><a href="chapter_0/chapter_0.html"><strong aria-hidden="true">1.</strong> Motivation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="chapter_0/chapter_0_0.html"><strong aria-hidden="true">1.1.</strong> A Simplest Example</a></li><li class="chapter-item expanded "><a href="chapter_0/chapter_0_1.html"><strong aria-hidden="true">1.2.</strong> A Simple Example</a></li><li class="chapter-item expanded "><a href="chapter_0/chapter_0_2.html"><strong aria-hidden="true">1.3.</strong> When to use Timely Dataflow</a></li><li class="chapter-item expanded "><a href="chapter_0/chapter_0_3.html"><strong aria-hidden="true">1.4.</strong> When not to use Timely Dataflow</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_1/chapter_1.html"><strong aria-hidden="true">2.</strong> Core Concepts</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="chapter_1/chapter_1_1.html"><strong aria-hidden="true">2.1.</strong> Dataflow</a></li><li class="chapter-item expanded "><a href="chapter_1/chapter_1_2.html"><strong aria-hidden="true">2.2.</strong> Timestamps</a></li><li class="chapter-item expanded "><a href="chapter_1/chapter_1_3.html"><strong aria-hidden="true">2.3.</strong> Progress</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_2/chapter_2.html"><strong aria-hidden="true">3.</strong> Building Timely Dataflows</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="chapter_2/chapter_2_1.html"><strong aria-hidden="true">3.1.</strong> Creating Inputs</a></li><li class="chapter-item expanded "><a href="chapter_2/chapter_2_2.html"><strong aria-hidden="true">3.2.</strong> Observing Outputs</a></li><li class="chapter-item expanded "><a href="chapter_2/chapter_2_3.html"><strong aria-hidden="true">3.3.</strong> Adding Operators</a></li><li class="chapter-item expanded "><a href="chapter_2/chapter_2_4.html"><strong aria-hidden="true">3.4.</strong> Creating Operators</a></li><li class="chapter-item expanded "><a href="chapter_2/chapter_2_5.html"><strong aria-hidden="true">3.5.</strong> A Worked Example</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_3/chapter_3.html"><strong aria-hidden="true">4.</strong> Running Timely Dataflows</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="chapter_3/chapter_3_1.html"><strong aria-hidden="true">4.1.</strong> Providing Input</a></li><li class="chapter-item expanded "><a href="chapter_3/chapter_3_2.html"><strong aria-hidden="true">4.2.</strong> Monitoring Probes</a></li><li class="chapter-item expanded "><a href="chapter_3/chapter_3_3.html"><strong aria-hidden="true">4.3.</strong> Operator Execution</a></li><li class="chapter-item expanded "><a href="chapter_3/chapter_3_4.html"><strong aria-hidden="true">4.4.</strong> Extending Dataflows</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_4/chapter_4.html"><strong aria-hidden="true">5.</strong> Advanced Timely Dataflow</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="chapter_4/chapter_4_1.html"><strong aria-hidden="true">5.1.</strong> Scopes</a></li><li class="chapter-item expanded "><a href="chapter_4/chapter_4_2.html"><strong aria-hidden="true">5.2.</strong> Iteration</a></li><li class="chapter-item expanded "><a href="chapter_4/chapter_4_3.html"><strong aria-hidden="true">5.3.</strong> Flow Control</a></li><li class="chapter-item expanded "><a href="chapter_4/chapter_4_4.html"><strong aria-hidden="true">5.4.</strong> Capture and Replay</a></li><li class="chapter-item expanded "><a href="chapter_4/chapter_4_5.html"><strong aria-hidden="true">5.5.</strong> Custom Datatypes</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_5/chapter_5.html"><strong aria-hidden="true">6.</strong> Internals</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="chapter_5/chapter_5_1.html"><strong aria-hidden="true">6.1.</strong> Communication</a></li><li class="chapter-item expanded "><a href="chapter_5/chapter_5_2.html"><strong aria-hidden="true">6.2.</strong> Progress Tracking</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#timely-dataflow" id="timely-dataflow">Timely Dataflow</a></h1>
<p>In this book we will work through the motivation and technical details behind <a href="https://github.com/TimelyDataflow/timely-dataflow">timely dataflow</a>, which is both a system for implementing distributed streaming computation, and if you look at it right, a way to structure computation generally.</p>
<p>Timely dataflow arose from <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2013/11/naiad_sosp2013.pdf">work at Microsoft Research</a>, where a group of us worked on building scalable, distributed data processing platforms. Our experience was that other systems did not provide both <em>expressive computation</em> and <em>high performance</em>. Efficient systems would only let you write restricted programs, and expressive systems employed synchronous and otherwise inefficient execution.</p>
<p>Our goal was to provide a not-unpleasant experience where you could write sophisticated streaming computations (e.g. with iterative control flow), which nonetheless compile down to systems that execute with only a modicum of overhead and synchronization.</p>
<h1><a class="header" href="#motivation" id="motivation">Motivation</a></h1>
<p>Let's start with some motivation: what can you do with timely dataflow, and when should you be excited to use it as opposed to other programming frameworks? Is timely dataflow great for everything, or is it only great for a few things? Is it great for anything? We will try and clarify these questions in this section.</p>
<h2><a class="header" href="#a-simplest-example" id="a-simplest-example">A simplest example</a></h2>
<p>Let's start with what may be the simplest non-trivial timely dataflow program.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::{ToStream, Inspect};

fn main() {
    timely::example(|scope| {
        (0..10).to_stream(scope)
               .inspect(|x| println!(&quot;seen: {:?}&quot;, x));
    });
}
</code></pre></pre>
<p>This program gives us a bit of a flavor for what a timely dataflow program might look like, including a bit of what Rust looks like, without getting too bogged down in weird stream processing details. Not to worry; we will do that in just a moment!</p>
<p>If we run the program up above, we see it print out the numbers zero through nine.</p>
<pre><code class="language-ignore">    Echidnatron% cargo run --example simple
        Finished dev [unoptimized + debuginfo] target(s) in 0.05s
         Running `target/debug/examples/simple`
    seen: 0
    seen: 1
    seen: 2
    seen: 3
    seen: 4
    seen: 5
    seen: 6
    seen: 7
    seen: 8
    seen: 9
    Echidnatron%
</code></pre>
<p>This isn't very different from a Rust program that would do this much more simply, namely the program</p>
<pre><pre class="playground"><code class="language-rust">fn main() {
    (0..10).for_each(|x| println!(&quot;seen: {:?}&quot;, x));
}
</code></pre></pre>
<p>Why would we want to make our life so complicated? The main reason is that we can make our program <em>reactive</em>, so that we can run it without knowing ahead of time the data we will use, and it will respond as we produce new data.</p>
<h2><a class="header" href="#an-example" id="an-example">An example</a></h2>
<p>Timely dataflow means to capture a large number of idioms, so it is a bit tricky to wrap together one example that shows off all of its features, but let's look at something that shows off some core functionality to give a taste.</p>
<p>The following complete program initializes a timely dataflow computation, in which participants can supply a stream of numbers which are exchanged between the workers based on their value. Workers print to the screen when they see numbers. You can also find this as <a href="https://github.com/frankmcsherry/timely-dataflow/blob/master/examples/hello.rs"><code>examples/hello.rs</code></a> in the <a href="https://github.com/frankmcsherry/timely-dataflow/tree/master/examples">timely dataflow repository</a>.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::InputHandle;
use timely::dataflow::operators::{Input, Exchange, Inspect, Probe};

fn main() {
    // initializes and runs a timely dataflow.
    timely::execute_from_args(std::env::args(), |worker| {

        let index = worker.index();
        let mut input = InputHandle::new();

        // create a new input, exchange data, and inspect its output
        let probe = worker.dataflow(|scope|
            scope.input_from(&amp;mut input)
                 .exchange(|x| *x)
                 .inspect(move |x| println!(&quot;worker {}:\thello {}&quot;, index, x))
                 .probe()
        );

        // introduce data and watch!
        for round in 0..10 {
            if index == 0 {
                input.send(round);
            }
            input.advance_to(round + 1);
            while probe.less_than(input.time()) {
                worker.step();
            }
        }
    }).unwrap();
}
</code></pre></pre>
<p>We can run this program in a variety of configurations: with just a single worker thread, with one process and multiple worker threads, and with multiple processes each with multiple worker threads.</p>
<p>To try this out yourself, first clone the timely dataflow repository using <code>git</code></p>
<pre><code class="language-ignore">    Echidnatron% git clone https://github.com/frankmcsherry/timely-dataflow
    Cloning into 'timely-dataflow'...
    remote: Counting objects: 14254, done.
    remote: Compressing objects: 100% (2267/2267), done.
    remote: Total 14254 (delta 2625), reused 3824 (delta 2123), pack-reused 9856
    Receiving objects: 100% (14254/14254), 9.01 MiB | 1.04 MiB/s, done.
    Resolving deltas: 100% (10686/10686), done.
</code></pre>
<p>Now <code>cd</code> into the directory and build timely dataflow by typing</p>
<pre><code class="language-ignore">    Echidnatron% cd timely-dataflow
    Echidnatron% cargo build
        Updating registry `https://github.com/rust-lang/crates.io-index`
    Compiling timely_sort v0.1.6
    Compiling byteorder v0.4.2
    Compiling libc v0.2.29
    Compiling abomonation v0.4.5
    Compiling getopts v0.2.14
    Compiling time v0.1.38
    Compiling timely_communication v0.1.7
    Compiling timely v0.2.0 (file:///Users/mcsherry/Projects/temporary/timely-dataflow)
        Finished dev [unoptimized + debuginfo] target(s) in 6.37 secs
</code></pre>
<p>Now we build the <code>hello</code> example</p>
<pre><code class="language-ignore">    Echidnatron% cargo build --example hello
    Compiling rand v0.3.16
    Compiling timely v0.2.0 (file:///Users/mcsherry/Projects/temporary/timely-dataflow)
        Finished dev [unoptimized + debuginfo] target(s) in 6.35 secs
</code></pre>
<p>And finally we run the <code>hello</code> example</p>
<pre><code class="language-ignore">    Echidnatron% cargo run --example hello
        Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs
        Running `target/debug/examples/hello`
    worker 0:	hello 0
    worker 0:	hello 1
    worker 0:	hello 2
    worker 0:	hello 3
    worker 0:	hello 4
    worker 0:	hello 5
    worker 0:	hello 6
    worker 0:	hello 7
    worker 0:	hello 8
    worker 0:	hello 9
    Echidnatron%
</code></pre>
<p>Rust is relatively clever, and we could have skipped the <code>cargo build</code> and <code>cargo build --example hello</code> commands; just invoking <code>cargo run --example hello</code> will build (or rebuild) anything necessary.</p>
<p>Of course, we can run this with multiple workers using the <code>-w</code> or <code>--workers</code> flag, followed by the number of workers we want in the process. Notice that you'll need an <code>--</code> before the arguments to our program; any arguments before that are treated as arguments to the <code>cargo</code> command.</p>
<pre><code class="language-ignore">    Echidnatron% cargo run --example hello -- -w2
        Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs
        Running `target/debug/examples/hello -w2`
    worker 0:	hello 0
    worker 1:	hello 1
    worker 0:	hello 2
    worker 1:	hello 3
    worker 0:	hello 4
    worker 1:	hello 5
    worker 0:	hello 6
    worker 1:	hello 7
    worker 0:	hello 8
    worker 1:	hello 9
    Echidnatron%
</code></pre>
<p>Although you can't easily see this happening, timely dataflow has spun up <em>two</em> worker threads and together they have exchanged some data and printed the results as before. However, notice that the worker index is now varied; this is our only clue that different workers exist, and processed different pieces of data. Worker zero introduces all of the data (notice the guard in the code; without this <em>each</em> worker would introduce <code>0 .. 10</code>), and then it is shuffled between the workers. The only <em>guarantee</em> is that records that evaluate to the same integer in the exchange closure go to the same worker. In practice, we (currently) route records based on the remainder of the number when divided by the number of workers.</p>
<p>Finally, let's run with multiple processes. To do this, you use the <code>-n</code> and <code>-p</code> arguments, which tell each process how many total processes to expect (the <code>-n</code> parameter) and which index this process should identify as (the <code>-p</code> parameter). You can also use <code>-h</code> to specify a host file with names and ports of each of the processes involved, but if you leave it off timely defaults to using the local host.</p>
<p>In one shell, I'm going to start a computation that expects multiple processes. It will hang out waiting for the other processes to start up.</p>
<pre><code class="language-ignore">    Echidnatron% cargo run --example hello -- -n2 -p0
        Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs
        Running `target/debug/examples/hello -n2 -p0`
</code></pre>
<p>Now if we head over to another shell, we can type the same thing but with a different <code>-p</code> identifier.</p>
<pre><code class="language-ignore">    Echidnatron% cargo run --example hello -- -n2 -p1
        Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs
        Running `target/debug/examples/hello -n2 -p1`
    worker 1:	hello 1
    worker 1:	hello 3
    worker 1:	hello 5
    worker 1:	hello 7
    worker 1:	hello 9
    Echidnatron%
</code></pre>
<p>Wow, fast! And, we get to see some output too. Only the output for this worker, though. If we head back to the other shell we see the process got moving and produced the other half of the output.</p>
<pre><code class="language-ignore">    Echidnatron% cargo run --example hello -- -n2 -p0
        Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs
        Running `target/debug/examples/hello -n2 -p0`
    worker 0:	hello 0
    worker 0:	hello 2
    worker 0:	hello 4
    worker 0:	hello 6
    worker 0:	hello 8
    Echidnatron%
</code></pre>
<p>This may seem only slightly interesting so far, but we will progressively build up more interesting tools and more interesting computations, and see how timely dataflow can efficiently execute them for us.</p>
<h1><a class="header" href="#when-to-use-timely-dataflow" id="when-to-use-timely-dataflow">When to use Timely Dataflow</a></h1>
<p>Timely dataflow may be a different programming model than you are used to, but if you can adapt your program to it there are several benefits.</p>
<ul>
<li>
<p><strong>Data Parallelism</strong>: The operators in timely dataflow are largely &quot;data-parallel&quot;, meaning they can operate on independent parts of the data concurrently. This allows the underlying system to distribute timely dataflow computations across multiple parallel workers. These can be threads on your computer, or even threads across computers in a cluster you have access to. This distribution typically improves the throughput of the system, and lets you scale to larger problems with access to more resources (computation, communication, and memory).</p>
</li>
<li>
<p><strong>Streaming Data</strong>: The core data type in timely dataflow is a <em>stream</em> of data, an unbounded collection of data not all of which is available right now, but which instead arrives as the computation proceeds. Streams are a helpful generalization of static data sets, which are assumed available at the start of the computation. By expressing your program as a computation on streams, you've explained both how it should respond to static input data sets (feed all the data in at once) but also how it should react to new data that might arrive later on.</p>
</li>
<li>
<p><strong>Expressivity</strong>: Timely dataflow's main addition over traditional stream processors is its ability to express higher-level control constructs, like iteration. This moves stream computations from the limitations of straight line code to the world of <em>algorithms</em>. Many of the advantages of timely dataflow computations come from our ability to express a more intelligent algorithm than the alternative systems, which can only express more primitive computations.</p>
</li>
</ul>
<p>There are many domains where streaming and scalability are important, and I'm not sure I can name them all. If you would like to build a scalable monitoring application for a service you run, timely dataflow can express this. If you would like to work with big data computations processing more data than your computer can load into memory, timely dataflow streams can represent this efficiently. If you would like to build an incremental iterative computation over massive data (e.g. matrices, large graphs, text corpora), timely dataflow has done these things.</p>
<p>At the same time, dataflow computation is also another way of thinking about your program. Much like Rust causes you to think a bit harder about program structure, timely dataflow helps you tease out some structure to your program that results in a more effective computation. Even when writing something like <code>grep</code>, a program that scans lines of text looking for patterns, by stating your program as a dataflow computation its implementation immediately scales out to multiple threads, and even across multiple computers.</p>
<h2><a class="header" href="#generality" id="generality">Generality</a></h2>
<p>Is timely dataflow always applicable? The intent of this research project is to remove layers of abstraction fat that prevent you from expressing anything your computer can do efficiently in parallel.</p>
<p>Under the covers, your computer (the one on which you are reading this text) is a dataflow processor. When your computer <em>reads memory</em> it doesn't actually wander off to find the memory, it introduces a read request into your memory controller, an independent component that will eventually return with the associated cache line. Your computer then gets back to work on whatever it was doing, hoping the responses from the controller return in a timely fashion.</p>
<p>Academically, I treat &quot;my computer can do this, but timely dataflow cannot&quot; as a bug. There are degrees, of course, and timely dataflow isn't on par with the processor's custom hardware designed to handle low level requests efficiently, but <em>algorithmically</em>, the goal is that anything you can do efficiently with a computer you should be able to express in timely dataflow.</p>
<h1><a class="header" href="#when-not-to-use-timely-dataflow" id="when-not-to-use-timely-dataflow">When not to use Timely Dataflow</a></h1>
<p>There are several reasons not to use timely dataflow, though many of them are <em>friction</em> about how your problem is probably expressed, rather than fundamental technical limitations. There are fundamental technical limitations too, of course.</p>
<p>I've collected a few examples here, but the list may grow with input and feedback.</p>
<hr />
<p>Timely dataflow is a <em>dataflow</em> system, and this means that at its core it likes to move data around. This makes life complicated when you would prefer not to move data, and instead move things like pointers and references to data that otherwise stays put.</p>
<p>For example, sorting a slice of data is a fundamental task and one that parallelizes. But, the task of sorting is traditionally viewed as transforming the data in a supplied slice, rather than sending the data to multiple workers and then announcing that it got sorted. The data really does need to end up in one place, one single pre-existing memory allocation, and timely dataflow is not great at problems that cannot be recast as the movement of data.</p>
<p>One could re-imagine the sorting process as moving data around, and indeed this is what happens when large clusters need to be brought to bear on such a task, but that doesn't help you at all if what you needed was to sort your single allocation. A library like <a href="https://github.com/nikomatsakis/rayon">Rayon</a> would almost surely be better suited to the task.</p>
<hr />
<p>Dataflow systems are also fundamentally about breaking apart the execution of your program into independently operating parts. However, many programs are correct only because some things happen <em>before</em> or <em>after</em> other things. A classic example is <a href="https://en.wikipedia.org/wiki/Depth-first_search">depth-first search</a> in a graph: although there is lots of work to do on small bits of data, it is crucial that the exploration of nodes reachable along a graph edge complete before the exploration of nodes reachable along the next graph edge.</p>
<p>Although there is plenty of active research on transforming algorithms from sequential to parallel, if you aren't clear on how to express your program as a dataflow program then timely dataflow may not be a great fit. At the very least, the first step would be &quot;fundamentally re-imagine your program&quot;, which can be a fine thing to do, but is perhaps not something you would have to do with your traditional program.</p>
<hr />
<p>Timely dataflow is in a bit of a weird space between language library and runtime system. This means that it doesn't quite have the stability guarantees a library might have (when you call <code>data.sort()</code> you don't think about &quot;what if it fails?&quot;), nor does it have the surrounding infrastructure of a <a href="https://www.microsoft.com/en-us/research/project/dryadlinq/">DryadLINQ</a> or <a href="https://spark.apache.org">Spark</a> style of experience. Part of this burden is simply passed to you, and this may be intolerable depending on your goals for your program.</p>
<h1><a class="header" href="#chapter-1-core-concepts" id="chapter-1-core-concepts">Chapter 1: Core Concepts</a></h1>
<p>Timely dataflow relies on two fundamental concepts: <strong>timestamps</strong> and <strong>dataflow</strong>, which together lead to the concept of <strong>progress</strong>. We will want to break down these concepts because they play a fundamental role in understanding how timely dataflow programs are structured.</p>
<h2><a class="header" href="#dataflow" id="dataflow">Dataflow</a></h2>
<p>Dataflow programming is fundamentally about describing your program as independent components, each of which operate in response to the availability of input data, as well as describing the connections between these components.</p>
<p>The most important part of dataflow programming is the <em>independence</em> of the components. When you write a dataflow program, you provide the computer with flexibility in how it executes your program. Rather than insisting on a specific sequence of instructions the computer should follow, the computer can work on each of the components as it sees fit, perhaps even sharing the work with other computers.</p>
<h2><a class="header" href="#timestamps" id="timestamps">Timestamps</a></h2>
<p>While we want to enjoy the benefits of dataflow programming, we still need to understand whether and how our computation progresses. In traditional imperative programming we could reason that because instructions happen in some order, then once we reach a certain point all work (of a certain type) must be done. Instead, we will tag the data that move through our dataflow with <em>timestamps</em>, indicating (roughly) when they would have happened in a sequential execution.</p>
<p>Timestamps play at least two roles in timely dataflow: they allow dataflow components to make sense of the otherwise unordered inputs they see (&quot;ah, I received the data in <em>this</em> order, but I should behave as if it arrived in <em>this</em> order&quot;), and they allow the user (and others) to reason about whether they have seen all of the data with a certain timestamp.</p>
<p>Timestamps allow us to introduce sequential structure into our program, without requiring actual sequential execution.</p>
<h2><a class="header" href="#progress" id="progress">Progress</a></h2>
<p>In a traditional imperative program, if we want to return the maximum of a set of numbers, we just scan all the numbers and return the maximum. We don't have to worry about whether we've considered <em>all</em> of the numbers yet, because the program makes sure not to provide an answer until it has consulted each number.</p>
<p>This simple task is much harder in a dataflow setting, where numbers arrive as input to a component that is tracking the maximum. Before releasing a number as output, the component must know if it has seen everything, as one more value could change its answer. But strictly speaking, nothing we've said so far about dataflow or timestamps provide any information about whether more data might arrive.</p>
<p>If we combine dataflow program structure with timestamped data in such a way that as data move along the dataflow their timestamps only increase, we are able to reason about the <em>progress</em> of our computation. More specifically, at any component in the dataflow, we can reason about which timestamps we may yet see in the future. Timestamps that are no longer possible are considered &quot;passed&quot;, and components can react to this information as they see fit.</p>
<p>Continual information about the progress of a computation is the only basis of coordination in timely dataflow, and is the lightest touch we could think of.</p>
<h1><a class="header" href="#dataflow-programming" id="dataflow-programming">Dataflow Programming</a></h1>
<p>Dataflow programming is fundamentally about describing your program as independent components, each of which operate in response to the availability of input data, as well as describing the connections between these components. This has several advantages, mostly in how it allows a computer to execute your program, but it can take a bit of thinking to re-imagine your imperative computation as a dataflow computation.</p>
<h2><a class="header" href="#an-example-1" id="an-example-1">An example</a></h2>
<p>Let's write an overly simple dataflow program. Remember our <code>examples/hello.rs</code> program? We are going to revisit that, but with some <strong>timestamp</strong> aspects removed. The goal is to get a sense for dataflow with all of its warts, and to get you excited for the next section where we bring back the timestamps. :)</p>
<p>Here is a reduced version of <code>examples/hello.rs</code> that just feeds data into our dataflow, without paying any attention to progress made. In particular, we have removed the <code>probe()</code> operation, the resulting <code>probe</code> variable, and the use of <code>probe</code> to determine how long we should step the worker before introducing more data.</p>
<pre><pre class="playground"><code class="language-rust">#![allow(unused_variables)]
extern crate timely;

use timely::dataflow::InputHandle;
use timely::dataflow::operators::{Input, Exchange, Inspect, Probe};

fn main() {
    // initializes and runs a timely dataflow.
    timely::execute_from_args(std::env::args(), |worker| {

        let index = worker.index();
        let mut input = InputHandle::new();

        // create a new input, exchange data, and inspect its output
        let probe = worker.dataflow(|scope|
            scope.input_from(&amp;mut input)
                 .exchange(|x| *x)
                 .inspect(move |x| println!(&quot;worker {}:\thello {}&quot;, index, x))
                 .probe()
        );

        // introduce data and watch!
        for round in 0..10 {
            if worker.index() == 0 {
                input.send(round);
            }
            input.advance_to(round + 1);
            // worker.step_while(|| probe.less_than(input.time()));
        }
    }).unwrap();
}
</code></pre></pre>
<p>This program is a <em>dataflow program</em>. There are two dataflow operators here, <code>exchange</code> and <code>inspect</code>, each of which is asked to do a thing in response to input data. The <code>exchange</code> operator takes each datum and hands it to a downstream worker based on the value it sees; with two workers, one will get all the even numbers and the other all the odd numbers. The <code>inspect</code> operator takes an action for each datum, in this case printing something to the screen.</p>
<p>Importantly, we haven't imposed any constraints on how these operators need to run. We removed the code that caused the input to be delayed until a certain amount of progress had been made, and it shows in the results when we run with more than one worker:</p>
<pre><code class="language-ignore">    Echidnatron% cargo run --example hello -- -w2
        Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs
        Running `target/debug/examples/hello -w2`
    worker 1:	hello 1
    worker 1:	hello 3
    worker 0:	hello 0
    worker 1:	hello 5
    worker 0:	hello 2
    worker 1:	hello 7
    worker 1:	hello 9
    worker 0:	hello 4
    worker 0:	hello 6
    worker 0:	hello 8
    Echidnatron%
</code></pre>
<p>What a mess. Nothing in our dataflow program requires that workers zero and one alternate printing to the screen, and you can even see that worker one is <em>done</em> before worker zero even gets to printing <code>hello 4</code>.</p>
<hr />
<p>However, this is only a mess if we are concerned about the order, and in many cases we are not. Imagine instead of just printing the number to the screen, we want to find out which numbers are prime and print <em>them</em> to the screen.</p>
<pre><code class="language-rust ignore">.inspect(|x| {
    // we only need to test factors up to sqrt(x)
    let limit = (*x as f64).sqrt() as u64;
    if *x &gt; 1 &amp;&amp; (2 .. limit + 1).all(|i| x % i &gt; 0) {
        println!(&quot;{} is prime&quot;, x);
    }
})
</code></pre>
<p>We don't really care that much about the order (we just want the results), and we have written such a simple primality test that we are going to be thrilled if we can distribute the work across multiple cores.</p>
<p>Let's check out the time to print out the prime numbers up to 10,000 using one worker:</p>
<pre><code class="language-ignore">    Echidnatron% time cargo run --example hello -- -w1 &gt; output1.txt
        Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs
        Running `target/debug/examples/hello -w1`
    cargo run --example hello -- -w1 &gt; output1.txt  59.84s user 0.10s system 99% cpu 1:00.01 total
    Echidnatron%
</code></pre>
<p>And now again with two workers:</p>
<pre><code class="language-ignore">    Echidnatron% time cargo run --example hello -- -w2 &gt; output2.txt
        Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs
        Running `target/debug/examples/hello -w2`
    cargo run --example hello -- -w2 &gt; output2.txt  60.74s user 0.12s system 196% cpu 30.943 total
    Echidnatron%
</code></pre>
<p>The time is basically halved, from one minute to thirty seconds, which is a great result for those of us who like factoring small numbers. Furthermore, although the 1,262 lines of results of <code>output1.txt</code> and <code>output2.txt</code> are not in the same order, it takes a fraction of a second to make them so, and verify that they are identical:</p>
<pre><code class="language-ignore">    Echidnatron% sort output1.txt &gt; sorted1.txt
    Echidnatron% sort output2.txt &gt; sorted2.txt
    Echidnatron% diff sorted1.txt sorted2.txt
    Echidnatron%
</code></pre>
<hr />
<p>This is probably as good a time as any to tell you about Rust's <code>--release</code> flag. I haven't been using it up above to keep things simple, but adding the <code>--release</code> flag to cargo's arguments makes the compilation take a little longer, but the resulting program run a <em>lot</em> faster. Let's do that now, to get a sense for how much of a difference it makes:</p>
<pre><code class="language-ignore">    Echidnatron% time cargo run --release --example hello -- -w1 &gt; output1.txt
        Finished release [optimized] target(s) in 0.0 secs
        Running `target/release/examples/hello -w1`
    cargo run --release --example hello -- -w1 &gt; output1.txt  0.78s user 0.06s system 96% cpu 0.881 total
    Echidnatron% time cargo run --release --example hello -- -w2 &gt; output2.txt
        Finished release [optimized] target(s) in 0.0 secs
        Running `target/release/examples/hello -w2`
    cargo run --release --example hello -- -w2 &gt; output2.txt  0.73s user 0.05s system 165% cpu 0.474 total
</code></pre>
<p>That is about a 60x speed-up. The good news is that we are still getting approximately a 2x speed-up going from one worker to two, but you can see that dataflow programming does not magically extract all performance from your computer.</p>
<p>This is also a fine time to point out that dataflow programming is not religion. There is an important part of our program up above that is imperative:</p>
<pre><code class="language-ignore rust">    let limit = (*x as f64).sqrt() as u64;
    if *x &gt; 1 &amp;&amp; (2 .. limit + 1).all(|i| x % i &gt; 0) {
        println!(&quot;{} is prime&quot;, x);
    }
</code></pre>
<p>This is an imperative fragment telling the <code>inspect</code> operator what to do. We <em>could</em> write this as a dataflow fragment if we wanted, but it is frustrating to do so, and less efficient. The control flow fragment lets us do something important, something that dataflow is bad at: the <code>all</code> method above <em>stops</em> as soon as it sees a factor of <code>x</code>.</p>
<p>There is a time and a place for dataflow programming and for control flow programming. We are going to try and get the best of both.</p>
<h1><a class="header" href="#logical-timestamps" id="logical-timestamps">Logical Timestamps</a></h1>
<p>When dataflow programs move data around arbitrarily, it becomes hard to correlate the produced outputs with the supplied inputs. If we supply a stream of bank transactions as input, and the output is a stream of bank balances, how can we know which input transactions are reflected in which output balances?</p>
<p>The standard approach to this problem is to install <em>timestamps</em> on the data. Each record gets a logical timestamp associated with it that indicates <em>when</em> it should be thought to happen. This is not necessarily &quot;when&quot; in terms of the date, time, or specific nanosecond the record was emitted; a timestamp could simply be a sequence number identifying a batch of input records. Or, and we will get into the terrifying details later, it could be much more complicated than this.</p>
<p>Timestamps are what allow us to correlate inputs and outputs. When we introduce records with some logical timestamp, unless our dataflow computation changes the timestamps, we expect to see corresponding outputs with that same timestamp.</p>
<h2><a class="header" href="#an-example-2" id="an-example-2">An example</a></h2>
<p>Remember from the dataflow section how when we remove the coordination from our <code>examples/hello.rs</code> program, the output was produced in some horrible order? In fact, each of those records had a timestamp associated with it that would reveal the correct order; we just weren't printing the timestamp because <code>inspect</code> doesn't have access to it.</p>
<p>Let's change the program to print out the timestamp with each record. This shouldn't be a very thrilling output, because the timestamp is exactly the same as the number itself, but that didn't have to be the case. We are just going to replace the line</p>
<pre><code class="language-rust ignore">.inspect(move |x| println!(&quot;worker {}:\thello {}&quot;, index, x))
</code></pre>
<p>with a slightly more complicated operator, <code>inspect_batch</code>.</p>
<pre><code class="language-rust ignore">.inspect_batch(move |t,xs| {
    for x in xs.iter() {
        println!(&quot;worker {}:\thello {} @ {:?}&quot;, index, x, t)
    }
})
</code></pre>
<p>The <code>inspect_batch</code> operator gets lower-level access to data in timely dataflow, in particular access to batches of records with the same timestamp. It is intended for diagnosing system-level details, but we can also use it to see what timestamps accompany the data.</p>
<p>The output we get with two workers is now:</p>
<pre><code class="language-ignore">    Echidnatron% cargo run --example hello -- -w2
        Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs
        Running `target/debug/examples/hello -w2`
    worker 1:	hello 1 @ (Root, 1)
    worker 1:	hello 3 @ (Root, 3)
    worker 1:	hello 5 @ (Root, 5)
    worker 0:	hello 0 @ (Root, 0)
    worker 0:	hello 2 @ (Root, 2)
    worker 0:	hello 4 @ (Root, 4)
    worker 0:	hello 6 @ (Root, 6)
    worker 0:	hello 8 @ (Root, 8)
    worker 1:	hello 7 @ (Root, 7)
    worker 1:	hello 9 @ (Root, 9)
    Echidnatron%
</code></pre>
<p>The timestamps are the <code>(Root, i)</code> things for various values of <code>i</code>. These happen to correspond to the data themselves, but had we provided random input data rather than <code>i</code> itself we would still be able to make sense of the output and put it back &quot;in order&quot;.</p>
<h2><a class="header" href="#timestamps-for-dataflow-operators" id="timestamps-for-dataflow-operators">Timestamps for dataflow operators</a></h2>
<p>Timestamps are not only helpful for dataflow users, but for the operators themselves. With time we will start to write more interesting dataflow operators, and it may be important for them to understand which records should be thought to come before others.</p>
<p>Imagine, for example, a dataflow operator whose job is to report the &quot;sum so far&quot;, where &quot;so far&quot; should be with respect to the timestamp (as opposed to whatever arbitrary order the operator receives the records). Such an operator can't simply take its input records, add them to a total, and produce the result. The input records may no longer be ordered by timestamp, and the produced summations may not reflect any partial sum of the input. Instead, the operator needs to look at the timestamps on the records, and incorporate the numbers in order of their timestamps.</p>
<p>Of course, such an operator works great as long as it expects exactly one record for each timestamp. Things get harder for it if it might receive multiple records at each timestamp, or perhaps none. To address this, the underlying system will have to help the operator reason about the progress of its input, up next.</p>
<h1><a class="header" href="#tracking-progress" id="tracking-progress">Tracking Progress</a></h1>
<p>Both dataflow and timestamps are valuable in their own right, but when we bring them together we get something even better. We get the ability to reason about the flow of timestamps through our computation, and we recover the ability to inform each dataflow component about how much of its input data it has seen.</p>
<p>Let's recall that bit of code we commented out from <code>examples/hello.rs</code>, which had to do with consulting something named <code>probe</code>.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::InputHandle;
use timely::dataflow::operators::{Input, Exchange, Inspect, Probe};

fn main() {
    // initializes and runs a timely dataflow.
    timely::execute_from_args(std::env::args(), |worker| {

        let index = worker.index();
        let mut input = InputHandle::new();

        // create a new input, exchange data, and inspect its output
        let probe = worker.dataflow(|scope|
            scope.input_from(&amp;mut input)
                 .exchange(|x| *x)
                 .inspect(move |x| println!(&quot;worker {}:\thello {}&quot;, index, x))
                 .probe()
        );

        // introduce data and watch!
        for round in 0..10 {
            if worker.index() == 0 {
                input.send(round);
            }
            input.advance_to(round + 1);
            worker.step_while(|| probe.less_than(input.time()));
        }
    }).unwrap();
}
</code></pre></pre>
<p>We'll put the whole program up here, but there are really just two lines that deal with progress tracking:</p>
<pre><code class="language-rust ignore">input.advance_to(round + 1);
worker.step_while(|| probe.less_than(input.time()));
</code></pre>
<p>Let's talk about each of them.</p>
<h2><a class="header" href="#input-capabilities" id="input-capabilities">Input capabilities</a></h2>
<p>The <code>input</code> structure is how we provide data to a timely dataflow computation, and it has a timestamp associated with it. Initially this timestamp is the default value, usually something like <code>0</code> for integers. Whatever timestamp <code>input</code> has, it can introduce data with that timestamp or greater. We can advance this timestamp, via the <code>advance_to</code> method, which restricts the timestamps we can use to those greater or equal to whatever timestamp is supplied as the argument.</p>
<p>The <code>advance_to</code> method is a big deal. This is the moment in the computation where our program reveals to the system, and through the system to all other dataflow workers, that we might soon be able to announce a timestamp as complete. There may still be records in flight bearing that timestamp, but as they are retired the system can finally report that progress has been made.</p>
<h2><a class="header" href="#output-possibilities" id="output-possibilities">Output possibilities</a></h2>
<p>The <code>probe</code> structure is how we learn about the possibility of timestamped data at some point in the dataflow graph. We can, at any point, consult a probe with the <code>less_than</code> method and ask whether it is still possible that we might see a time less than the argument at that point in the dataflow graph. There is also a <code>less_equal</code> method, if you prefer that.</p>
<p>Putting a probe after the <code>inspect</code> operator, which passes through all data it receives as input only after invoking its method, tells us whether we should expect to see the method associated with <code>inspect</code> fire again for a given timestamp. If we are told we won't see any more messages with timestamp <code>t</code> after the <code>inspect</code>, then the <code>inspect</code> won't see any either.</p>
<p>The <code>less_than</code> and <code>less_equal</code> methods are the only place where we learn about the state of the rest of the system. These methods are non-blocking; they always return immediately with either a &quot;yes, you might see such a timestamp&quot; or a &quot;no, you will not see such a timestamp&quot;.</p>
<h2><a class="header" href="#responding-to-progress-information" id="responding-to-progress-information">Responding to progress information</a></h2>
<p>Progress information is relatively passive. We get to observe what happens in the rest of the system, and perhaps change our behavior based on the amount of progress. We do not get to tell the system what to do next, we just get to see what has happened since last we checked.</p>
<p>This passive approach to coordination allows the system to operate with minimal overhead. Workers exchange both data and progress information. If workers want to wait for further progress before introducing more data they see they are welcome to do so, but they can also go and work on a different part of the dataflow graph as well.</p>
<p>Progress information provides a relatively unopinionated view of coordination. Workers are welcome to impose a more synchronous discipline using progress information, perhaps proceeding in sequence through operators by consulting probes installed after each of them, but they are not required to do so. Synchronization is possible, but it becomes a choice made by the workers themselves, rather than imposed on them by the system.</p>
<h1><a class="header" href="#building-timely-dataflows" id="building-timely-dataflows">Building Timely Dataflows</a></h1>
<p>Let's talk about how to create timely dataflows.</p>
<p>This section will be a bit of a tour through the dataflow construction process, ignoring for the moment details about the interesting ways in which you can get data into and out of your dataflow; those will show up in the &quot;Running Timely Dataflows&quot; section. For now we are going to work with examples with fixed input data and no interactivity to speak of, focusing on what we can cause to happen to that data.</p>
<p>Here is a relatively simple example, taken from <code>timely/examples/simple.rs</code>, that turns the numbers zero through nine into a stream, and then feeds them through an <code>inspect</code> operator printing them to the screen.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::{ToStream, Inspect};

fn main() {
    timely::example(|scope| {
        (0..10).to_stream(scope)
               .inspect(|x| println!(&quot;seen: {:?}&quot;, x));
    });
}
</code></pre></pre>
<p>We are going to develop out this example, showing off both the built-in operators as well as timely's generic operator construction features.</p>
<hr />
<p><strong>NOTE</strong>: Timely very much assumes that you are going to build the same dataflow on each worker. You don't literally have to, in that you could build a dataflow from user input, or with a random number generator, things like that. Please don't! It will not be a good use of your time.</p>
<h1><a class="header" href="#creating-inputs" id="creating-inputs">Creating Inputs</a></h1>
<p>Let's start with the first thing we'll want for a dataflow computation: a source of data.</p>
<p>Almost all operators in timely can only be defined from a source of data, with a few exceptions. One of these exceptions is the <code>to_stream</code> operator, which is defined for various types and which takes a <code>scope</code> as an argument and produces a stream in that scope. Our <code>InputHandle</code> type from previous examples has a <code>to_stream</code> method, as well as any type that can be turned into an iterator (which we used in the preceding example).</p>
<p>For example, we can create a new dataflow with one interactive input and one static input:</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::InputHandle;
use timely::dataflow::operators::ToStream;

fn main() {
    // initializes and runs a timely dataflow.
    timely::execute_from_args(std::env::args(), |worker| {

        let mut input = InputHandle::&lt;(), String&gt;::new();

        // define a new dataflow
        worker.dataflow(|scope| {

            let stream1 = input.to_stream(scope);
            let stream2 = (0 .. 9).to_stream(scope);

        });

    }).unwrap();
}
</code></pre></pre>
<p>There will be more to do to get data into <code>input</code>, and we aren't going to worry about that at the moment. But, now you know two of the places you can get data from!</p>
<h2><a class="header" href="#other-sources" id="other-sources">Other sources</a></h2>
<p>There are other sources of input that are a bit more advanced. Once we learn how to create custom operators, the <code>source</code> method will allow us to create a custom operator with zero input streams and one output stream, which looks like a source of data (hence the name). There are also the <code>Capture</code> and <code>Replay</code> traits that allow us to exfiltrate the contents of a stream from one dataflow (using <code>capture_into</code>) and re-load it in another dataflow (using <code>replay_from</code>).</p>
<h1><a class="header" href="#observing-outputs" id="observing-outputs">Observing Outputs</a></h1>
<p>Having constructed a minimal streaming computation, we might like to take a peek at the output. There are a few ways to do this, but the simplest by far is the <code>inspect</code> operator.</p>
<p>The <code>inspect</code> operator is called with a closure, and it ensures that the closure is run on each record that passes through the operator. This closure can do just about anything, from printing to the screen or writing to a file.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::{ToStream, Inspect};

fn main() {
    timely::execute_from_args(std::env::args(), |worker| {
        worker.dataflow::&lt;(),_,_&gt;(|scope| {
            (0 .. 9)
                .to_stream(scope)
                .inspect(|x| println!(&quot;hello: {}&quot;, x));
        });
    }).unwrap();
}
</code></pre></pre>
<p>This simple example turns the sequence zero through nine into a stream and then prints the results to the screen.</p>
<h2><a class="header" href="#inspecting-batches" id="inspecting-batches">Inspecting Batches</a></h2>
<p>The <code>inspect</code> operator has a big sibling, <code>inspect_batch</code>, whose closure gets access to whole batches of records at a time, just like the underlying operator. More precisely, <code>inspect_batch</code> takes a closure of two parameters: first, the timestamp of a batch, and second a reference to the batch itself. The <code>inspect_batch</code> operator can be especially helpful if you want to process the outputs more efficiently.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::{ToStream, Inspect};

fn main() {
    timely::execute_from_args(std::env::args(), |worker| {
        worker.dataflow::&lt;(),_,_&gt;(|scope| {
            (0 .. 10)
                .to_stream(scope)
                .inspect_batch(|t, xs| println!(&quot;hello: {:?} @ {:?}&quot;, xs, t));
        });
    }).unwrap();
}
</code></pre></pre>
<h2><a class="header" href="#capturing-streams" id="capturing-streams">Capturing Streams</a></h2>
<p>The <code>Capture</code> trait provides a mechanism for exfiltrating a stream from a dataflow, into information that can be replayed in other dataflows. The trait is pretty general, and can even capture a stream to a binary writer that can be read back from to reconstruct the stream (see <code>examples/capture_send.rs</code> and <code>examples/capture_recv.rs</code>).</p>
<p>The simplest form of capture is the <code>capture()</code> method, which turns the stream into a shared queue of &quot;events&quot;, which are the sequence of events the operator is exposed to: data arriving and notification of progress through the input stream. The <code>capture</code> method is used in many of timely's documentation tests, to extract a stream and verify that it is correct.</p>
<p>Consider the documentation test for the <code>ToStream</code> trait:</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::{ToStream, Capture};
use timely::dataflow::operators::capture::Extract;

fn main() {
    let (data1, data2) = timely::example(|scope| {
        let data1 = (0..3).to_stream(scope).capture();
        let data2 = vec![0,1,2].to_stream(scope).capture();
        (data1, data2)
    });

    assert_eq!(data1.extract(), data2.extract());
}
</code></pre></pre>
<p>Here the two <code>capture</code> methods each return the receive side of one of Rust's threadsafe channels. The data moving along the channel have a type <code>capture::Event&lt;T,D&gt;</code> which you would need to read about, but which your main thread can drain out of the channel and process as it sees fit.</p>
<h1><a class="header" href="#introducing-operators" id="introducing-operators">Introducing Operators</a></h1>
<p>In between introducing streams of data and inspecting or capturing the output, we'll probably want to do some computation on those data. There are a lot of things that you can do, and timely comes with a set of generally useful operators built in. We will survey a few of these, but this list will be necessarily incomplete: the operators are pretty easy to write, and keep showing up.</p>
<h2><a class="header" href="#mapping" id="mapping">Mapping</a></h2>
<p>One of the simplest things one can do with a stream of data is to transform each record into a new record. In database terminology this would be called &quot;projection&quot;, where you extract some fields from a larger record, but as we are in a more rich programming language we can perform arbitrary transformations.</p>
<p>The <code>map</code> operator takes as an argument a closure from the input data type to an output data type that you get to define. The result is the stream of records corresponding to the application of your closure to each record in the input stream.</p>
<p>The following program should print out the numbers one through ten.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::{ToStream, Inspect, Map};

fn main() {
    timely::execute_from_args(std::env::args(), |worker| {
        worker.dataflow::&lt;(),_,_&gt;(|scope| {
            (0 .. 9)
                .to_stream(scope)
                .map(|x| x + 1)
                .inspect(|x| println!(&quot;hello: {}&quot;, x));
        });
    }).unwrap();
}
</code></pre></pre>
<p>The closure <code>map</code> takes <em>owned</em> data as input, which means you are able to mutate it as you like without cloning or copying the data. For example, if you have a stream of <code>String</code> data, then you could upper-case the string contents without having to make a second copy; your closure owns the data that comes in, with all the benefits that entails.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::{ToStream, Inspect, Map};

fn main() {
    timely::execute_from_args(std::env::args(), |worker| {
        worker.dataflow::&lt;(),_,_&gt;(|scope| {
            (0 .. 9)
                .to_stream(scope)
                .map(|x| x.to_string())
                .map(|mut x| { x.truncate(5); x } )
                .inspect(|x| println!(&quot;hello: {}&quot;, x));
        });
    }).unwrap();
}
</code></pre></pre>
<h3><a class="header" href="#map-variants" id="map-variants">Map variants</a></h3>
<p>There are a few variants of <code>map</code> with different functionality.</p>
<p>For example, the <code>map_in_place</code> method takes a closure which receives a mutable reference and produces no output; instead, this method allows you to change the data <em>in-place</em>, which can be a valuable way to avoid duplication of resources.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::{ToStream, Inspect, Map};

fn main() {
    timely::execute_from_args(std::env::args(), |worker| {
        worker.dataflow::&lt;(),_,_&gt;(|scope| {
            (0 .. 9)
                .to_stream(scope)
                .map(|x| x.to_string())
                .map_in_place(|x| x.truncate(5))
                .inspect(|x| println!(&quot;hello: {}&quot;, x));
        });
    }).unwrap();
}
</code></pre></pre>
<p>Alternately, the <code>flat_map</code> method takes input data and allows your closure to transform each element to an iterator, which it then enumerates into the output stream. The following fragment takes each number from zero through eight and has each produce all numbers less than it. The result should be 8 zeros, 7 ones, and so on up to 1 seven.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::{ToStream, Inspect, Map};

fn main() {
    timely::execute_from_args(std::env::args(), |worker| {
        worker.dataflow::&lt;(),_,_&gt;(|scope| {
            (0 .. 9)
                .to_stream(scope)
                .flat_map(|x| 0 .. x)
                .inspect(|x| println!(&quot;hello: {}&quot;, x));
        });
    }).unwrap();
}
</code></pre></pre>
<h2><a class="header" href="#filtering" id="filtering">Filtering</a></h2>
<p>Another fundamental operation is <em>filtering</em>, in which a predicate dictates a subset of the stream to retain.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::{ToStream, Inspect, Filter};

fn main() {
    timely::execute_from_args(std::env::args(), |worker| {
        worker.dataflow::&lt;(),_,_&gt;(|scope| {
            (0 .. 9)
                .to_stream(scope)
                .filter(|x| *x % 2 == 0)
                .inspect(|x| println!(&quot;hello: {}&quot;, x));
        });
    }).unwrap();
}
</code></pre></pre>
<p>Unlike <code>map</code>, the predicate passed to the <code>filter</code> operator does not receive <em>owned</em> data, but rather a reference to the data. This allows <code>filter</code> to observe the data to determine whether to keep it, but not to change it.</p>
<h2><a class="header" href="#logical-partitioning" id="logical-partitioning">Logical Partitioning</a></h2>
<p>There are two operators for splitting and combining streams, <code>partition</code> and <code>concat</code> respectively.</p>
<p>The <code>partition</code> operator takes two arguments, a number of resulting streams to produce, and a closure which takes each record to a pair of the target partition identifier and the output record. The output of <code>partition</code> is a list of streams, where each stream contains those elements mapped to the stream under the closure.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::{ToStream, Partition, Inspect};

fn main() {
    timely::example(|scope| {
        let streams = (0..10).to_stream(scope)
                             .partition(3, |x| (x % 3, x));

        streams[0].inspect(|x| println!(&quot;seen 0: {:?}&quot;, x));
        streams[1].inspect(|x| println!(&quot;seen 1: {:?}&quot;, x));
        streams[2].inspect(|x| println!(&quot;seen 2: {:?}&quot;, x));
    });
}
</code></pre></pre>
<p>This example breaks the input stream apart into three logical streams, which are then subjected to different <code>inspect</code> operators. Importantly, <code>partition</code> only <em>logically</em> partitions the data, it does not move the data between workers. In the example above, each worker partitions its stream into three parts and no data are exchanged at all (as <code>inspect</code> does not require that of its inputs).</p>
<p>In the other direction, <code>concat</code> takes two streams and produces one output stream containing elements sent along either. The following example merges the partitioned streams back together.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::{ToStream, Partition, Concat, Inspect};

fn main() {
    timely::example(|scope| {
        let streams = (0..10).to_stream(scope)
                             .partition(3, |x| (x % 3, x));
        streams[0]
            .concat(&amp;streams[1])
            .concat(&amp;streams[2])
            .inspect(|x| println!(&quot;seen: {:?}&quot;, x));
    });
}
</code></pre></pre>
<p>There is also a <code>concatenate</code> method defined for scopes which collects all streams from a supplied vector, effectively undoing the work of <code>partition</code> in one operator.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::{ToStream, Partition, Concatenate, Inspect};

fn main() {
    timely::example(|scope| {
        let streams = (0..10).to_stream(scope)
                             .partition(3, |x| (x % 3, x));

        scope.concatenate(streams)
             .inspect(|x| println!(&quot;seen: {:?}&quot;, x));
    });
}
</code></pre></pre>
<p>Both <code>concat</code> and <code>concatenate</code> are efficient operations that exchange no data between workers, operate only on batches of stream elements, and do not make further copies of the data.</p>
<h2><a class="header" href="#physical-partitioning" id="physical-partitioning">Physical Partitioning</a></h2>
<p>To complement the logical partitioning of <code>partition</code>, timely also provides the physical partitioning operator <code>exchange</code> which routes records to a worker based on a supplied closure. The <code>exchange</code> operator does not change the contents of the stream, but rather the distribution of elements to the workers. This operation can be important if you would like to collect records before printing statistics to the screen, or otherwise do some work that requires a specific data distribution.</p>
<p>Operators that require a specific data distribution will ensure that this occurs as part of their definition. As the programmer, you should not need to invoke <code>exchange</code>.</p>
<p>There are times where <code>exchange</code> can be useful. For example, if a stream is used by two operators requiring the same distribution, simply using the stream twice will cause duplicate data exchange as each operator satisfies its requirements. Instead, it may make sense to invoke <code>exchange</code> to move the data once, at which point the two operators will no longer require serialization and communication to shuffle their inputs appropriately.</p>
<h2><a class="header" href="#other-operators" id="other-operators">Other operators</a></h2>
<p>There are any number of other operators, most of which you should be able to find in the <code>timely::dataflow::operators</code> module. Scanning through the documentation for this module may lead you to operators that you need, and alternately their implementations may demonstrate how to <em>construct</em> similar operators, if the one you require is not present. Operator construction is the subject of the next section!</p>
<h1><a class="header" href="#creating-operators" id="creating-operators">Creating Operators</a></h1>
<p>What if there isn't an operator that does what you want to do? What if what you want to do is better written as imperative code rather than a tangle of dataflow operators? Not a problem! Timely dataflow has you covered.</p>
<p>Timely has several &quot;generic&quot; dataflow operators that are pretty much ready to run, except someone (you) needs to supply their implementation. This isn't as scary as it sounds; you just need to write a closure that says &quot;given a handle to my inputs and outputs, what do I do when timely asks me to run?&quot;.</p>
<p>Let's look at an example</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::ToStream;
use timely::dataflow::operators::generic::operator::Operator;
use timely::dataflow::channels::pact::Pipeline;

fn main() {
    timely::example(|scope| {
        (0u64..10)
            .to_stream(scope)
            .unary(Pipeline, &quot;increment&quot;, |capability, info| {

                let mut vector = Vec::new();
                move |input, output| {
                    while let Some((time, data)) = input.next() {
                        data.swap(&amp;mut vector);
                        let mut session = output.session(&amp;time);
                        for datum in vector.drain(..) {
                            session.give(datum + 1);
                        }
                    }
                }
            });
    });
}
</code></pre></pre>
<p>What is going on here? The heart of the mess is the dataflow operator <code>unary</code>, which is a ready-to-assemble dataflow operator with one input and one output. The <code>unary</code> operator takes three arguments (it looks like so many more!): (i) instructions about how it should distribute its inputs, (ii) a tasteful name, and (iii) the logic it should execute whenever timely gives it a chance to do things.</p>
<p>Most of what is interesting lies in the closure, so let's first tidy up some loose ends before we dive in there. There are a few ways to request how input data should be distributed and <code>Pipeline</code> is the one that says &quot;don't move anything&quot;. The string &quot;increment&quot; is utterly arbitrary; this happens to be what the operator does, but you could change it to be your name, or a naughty word, or whatever you like. The <code>|capability|</code> stuff should be ignored for the moment; we'll explain in just a moment (it has to do with whether you would like the ability to send data before you receive any).</p>
<p>The heart of the logic lies in the closure that binds <code>input</code> and <code>output</code>. These two are handles respectively to the operator's input (from which it can read records) and the operator's output (to which it can send records).</p>
<p>The input handle <code>input</code> has one primary method, <code>next</code>, which may return a pair consisting of a <code>CapabilityRef&lt;Timestamp&gt;</code> and a batch of data. Rust really likes you to demonstrate a commitment to only looking at valid data, and our <code>while</code> loop does what is called deconstruction: we acknowledge the optional structure and only execute in the case the <code>Option</code> variant is <code>Some</code>, containing data. The <code>next</code> method could also return <code>None</code>, indicating that there is no more data available at the moment. It is strongly recommended that you take the hint and stop trying to read inputs at that point; timely gives you the courtesy of executing whatever code you want in this closure, but if you never release control back to the system you'll break things (timely employs <a href="https://en.wikipedia.org/wiki/Cooperative_multitasking">&quot;cooperative multitasking&quot;</a>).</p>
<p>The output handle <code>output</code> has one primary method, <code>session</code>, which starts up an output session at the indicated time. The resulting session can be given data in various ways: (i) element at a time with <code>give</code>, (ii) iterator at a time with <code>give_iterator</code>, and (iii) vector at a time with <code>give_content</code>. Internally it is buffering up the output and flushing automatically when the session goes out of scope, which happens above when we go around the <code>while</code> loop.</p>
<h3><a class="header" href="#other-shapes" id="other-shapes">Other shapes</a></h3>
<p>The <code>unary</code> method is handy if you have one input and one output. What if you want something with two inputs? Or what about zero inputs? We've still got you covered.</p>
<p>There is a <code>binary</code> method which looks a lot like unary, except that it has twice as many inputs (and ways to distribute the inputs), and requires a closure accepting two inputs and one output. You still get to write arbitrary code to drive the operator around as you like.</p>
<p>There is also a method <code>operators::source</code> which .. has no inputs. You can't call it on a stream, for obvious reasons, but you call it with a scope as an argument. It looks just like the other methods, except you supply a closure that just takes an output as an argument and sends whatever it wants each time it gets called. This is great for reading from external sources and moving data along as you like.</p>
<h3><a class="header" href="#capabilities" id="capabilities">Capabilities</a></h3>
<p>We skipped a discussion of the <code>capability</code> argument, and we need to dig into that now.</p>
<p>One of timely dataflow's main features is its ability to track whether an operator may or may not in the future receive more records bearing a certain timestamp. The way that timely does this is by requiring that its operators, like the ones we have written, hold <em>capabilities</em> for sending data at any timestamp. A capability is an instance of the <code>Capability&lt;Time&gt;</code> type, which looks to the outside world like an instance of <code>Time</code>, but which <code>output</code> will demand to see before it allows you to create a session.</p>
<p>Remember up where we got things we called <code>time</code> and from which we created a session with <code>session(&amp;time)</code>? That type was actually a capability.</p>
<p>Likewise, the <code>capability</code> argument that we basically ignored is also a capability. It is a capability for the default value of <code>Time</code>, from which one can send data at any timestamp. All operators get one of these to start out with, and until they downgrade or discard them, they retain the ability to send records at any time. The flip side of this is that the system doesn't make any progress <em>until</em> the operator downgrades or discards the capability.</p>
<p>The <code>capability</code> argument exists so that we can construct operators with the ability to send data before they receive any data. This is occasionally important for <code>unary</code> and <code>binary</code> operators, but it is <em>crucially important</em> for operators with no inputs. If we want to create an operator that reads from an external source and sends data, we'll need to keep hold of some capability.</p>
<p>Here is an example <code>source</code> implementation that produces all numbers up to some limit, each at a distinct time.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::Inspect;
use timely::dataflow::operators::generic::operator::source;

fn main() {
    timely::example(|scope| {

        source(scope, &quot;Source&quot;, |capability, info| {

            // Acquire a re-activator for this operator.
            use timely::scheduling::Scheduler;
            let activator = scope.activator_for(&amp;info.address[..]);

            let mut cap = Some(capability);
            move |output| {

                let mut done = false;
                if let Some(cap) = cap.as_mut() {

                    // get some data and send it.
                    let time = cap.time().clone();
                    output.session(&amp;cap)
                          .give(*cap.time());

                    // downgrade capability.
                    cap.downgrade(&amp;(time + 1));
                    done = time &gt; 20;
                }

                if done { cap = None; }
                else    { activator.activate(); }
            }
        })
        .inspect(|x| println!(&quot;number: {:?}&quot;, x));
    });
}
</code></pre></pre>
<p>The details seem a bit tedious, but let's talk them out. The first thing we do is capture <code>capability</code> in the variable <code>cap</code>, whose type is <code>Option&lt;Capability&lt;Time&gt;&gt;</code>. This type is important because it will allow us to eventually discard the capability, replacing it with <code>None</code>. If we always held a <code>Capability&lt;Time&gt;</code>, the best we could do would be to continually downgrade it. Another option is <code>Vec&lt;Capability&lt;Time&gt;&gt;</code>, which we could eventually clear.</p>
<p>Our next step is to define and return a closure that takes <code>output</code> as a parameter. The <code>move</code> keyword is part of Rust and is an important part of making sure that <code>cap</code> makes its way into the closure, rather than just evaporating from the local scope when we return.</p>
<p>The closure does a bit of a dance to capture the current time (not a capability, in this case), create a session with this time and send whatever the time happens to be as data, then downgrade the capability to be one timestep in the future. If it turns out that this is greater than twenty we discard the capability.</p>
<p>The system is smart enough to notice when you downgrade and discard capabilities, and it understands that these actions represent irreversible actions on your part that can now be communicated to others in the dataflow. As this closure is repeatedly executed, the timestamp of the capability will advance and the system will be able to indicate this to downstream operators.</p>
<h3><a class="header" href="#stateful-operators" id="stateful-operators">Stateful operators</a></h3>
<p>It may seem that we have only considered stateless operators, those that are only able to read from their inputs and immediately write to their outputs. But, you can have whatever state that you like, using the magic of Rust's closures. When we write a closure, it can capture (&quot;close over&quot;) any state that is currently in scope, taking ownership of it. This is actually what we did up above with the capability. If that sounds too abstract, let's look at an example.</p>
<p>Our <code>unary</code> example from way back just incremented the value and passed it along. What if we wanted to only pass values larger than any value we have seen so far? We just define a variable <code>max</code> which we check and update as we would normally. Importantly, we should define it <em>outside</em> the closure we return, so that it persists across calls, and we need to use the <code>move</code> keyword so that the closure knows it is supposed to take ownership of the variable.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::ToStream;
use timely::dataflow::operators::generic::operator::Operator;
use timely::dataflow::channels::pact::Pipeline;

fn main() {
    timely::example(|scope| {
        (0u64..10)
            .to_stream(scope)
            .unary(Pipeline, &quot;increment&quot;, |capability, info| {

                let mut maximum = 0;    // define this here; use in the closure
                let mut vector = Vec::new();

                move |input, output| {
                    while let Some((time, data)) = input.next() {
                        data.swap(&amp;mut vector);
                        let mut session = output.session(&amp;time);
                        for datum in vector.drain(..) {
                            if datum &gt; maximum {
                                session.give(datum + 1);
                                maximum = datum;
                            }
                        }
                    }
                }
            });
    });
}
</code></pre></pre>
<p>This example just captures an integer, but you could just as easily define and capture ownership of a <code>HashMap</code>, or whatever complicated state you would like repeated access to.</p>
<p>Bear in mind that this example is probably a bit wrong, in that we update <code>max</code> without paying any attention to the times of the data that come past, and so we may report a sequence of values that doesn't seem to correspond with the sequence when sorted by time. Writing sane operators in the presence of batches of data at shuffled times requires more thought. Specifically, for an operator to put its input back in order it needs to understand which times it might see in the future, which was the reason we were so careful about those capabilities and is the subject of the next subsection.</p>
<h3><a class="header" href="#frontiered-operators" id="frontiered-operators">Frontiered operators</a></h3>
<p>Timely dataflow is constantly tracking the capabilities of operators throughout the dataflow graph, and it reports this information to operators through what are called &quot;frontiers&quot;. Each input has an associated frontier, which is a description of the timestamps that might arrive on that input in the future.</p>
<p>Specifically, each input has a <code>frontier</code> method which returns a <code>&amp;[Timestamp]</code>, indicating a list of times such that any future time must be greater or equal to some element of the list. Often this list will just have a single element, indicating the &quot;current&quot; time, but as we get to more complicated forms of time (&quot;partially ordered&quot; time, if that means anything to you yet) we may need to report multiple incomparable timestamps.</p>
<p>This frontier information is invaluable for operators that must be sure that their output is correct and final before they send it as output. For our <code>max</code> example, we will want to wait to apply the new maximum until we are sure that we will not see any more elements at earlier times. That isn't to say we can't do anything with data we receive &quot;early&quot;; in the case of the maximum, each batch at a given time can be reduced down to just its maximum value, as all received values would be applied simultaneously.</p>
<p>To make life easier for you, we've written a helper type called <code>Notificator</code> whose job in life is to help you keep track of times that you would like to send outputs, and to tell you when (according to your input frontiers) it is now safe to send the data. In fact, notificators do more by holding on to the <em>capabilities</em> for you, so that you can be sure that, even if you <em>don't</em> receive any more messages but just an indication that there will be none, you will still retain the ability to send your messages.</p>
<p>Here is a worked example where we use a binary operator that implements the behavior of <code>concat</code>, but it puts its inputs in order, buffering its inputs until their associated timestamp is complete, and then sending all data at that time. The operator defines and captures a <code>HashMap&lt;Time, Vec&lt;Data&gt;&gt;</code> named <code>stash</code> which it uses to buffer received input data that are not yet ready to send.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use std::collections::HashMap;
use timely::dataflow::operators::{ToStream, FrontierNotificator};
use timely::dataflow::operators::generic::operator::Operator;
use timely::dataflow::channels::pact::Pipeline;

fn main() {
    timely::example(|scope| {

        let in1 = (0 .. 10).to_stream(scope);
        let in2 = (0 .. 10).to_stream(scope);

        in1.binary_frontier(&amp;in2, Pipeline, Pipeline, &quot;concat_buffer&quot;, |capability, info| {

            let mut notificator = FrontierNotificator::new();
            let mut stash = HashMap::new();

            move |input1, input2, output| {
                while let Some((time, data)) = input1.next() {
                    stash.entry(time.time().clone())
                         .or_insert(Vec::new())
                         .push(data.replace(Vec::new()));
                    notificator.notify_at(time.retain());
                }
                while let Some((time, data)) = input2.next() {
                    stash.entry(time.time().clone())
                         .or_insert(Vec::new())
                         .push(data.replace(Vec::new()));
                    notificator.notify_at(time.retain());
                }

                notificator.for_each(&amp;[input1.frontier(), input2.frontier()], |time, notificator| {
                    let mut session = output.session(&amp;time);
                    if let Some(list) = stash.remove(time.time()) {
                        for mut vector in list.into_iter() {
                            session.give_vec(&amp;mut vector);
                        }
                    }
                });
            }
        });
    });
}
</code></pre></pre>
<p>As an exercise, this example could be improved in a few ways. How might you change it so that the data are still sent in the order they are received, but messages may be sent as soon as they are received if their time is currently in the frontier? This would avoid buffering messages that are ready to go, and would only buffer messages that are out-of-order, potentially reducing the memory footprint and improving the effective latency.</p>
<p>Before ending the section, let's rewrite this example without the <code>notificator</code>, in an attempt to demystify how it works. Whether you use a notificator or not is up to you; they are mostly about staying sane in what can be a confusing setting, and you can totally skip them once you have internalized how capabilities and frontiers work.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use std::collections::HashMap;
use timely::dataflow::operators::{ToStream, FrontierNotificator};
use timely::dataflow::operators::generic::operator::Operator;
use timely::dataflow::channels::pact::Pipeline;

fn main() {
    timely::example(|scope| {

        let in1 = (0 .. 10).to_stream(scope);
        let in2 = (0 .. 10).to_stream(scope);

        in1.binary_frontier(&amp;in2, Pipeline, Pipeline, &quot;concat_buffer&quot;, |capability, info| {

            let mut stash = HashMap::new();

            move |input1, input2, output| {

                while let Some((time, data)) = input1.next() {
                    stash.entry(time.retain())
                         .or_insert(Vec::new())
                         .push(data.replace(Vec::new()));
                }
                while let Some((time, data)) = input2.next() {
                    stash.entry(time.retain())
                         .or_insert(Vec::new())
                         .push(data.replace(Vec::new()));
                }

                // consider sending everything in `stash`.
                let frontiers = &amp;[input1.frontier(), input2.frontier()];
                for (time, list) in stash.iter_mut() {
                    // if neither input can produce data at `time`, ship `list`.
                    if frontiers.iter().all(|f| !f.less_equal(time.time())) {
                        let mut session = output.session(&amp;time);
                        for mut vector in list.drain(..) {
                            session.give_vec(&amp;mut vector);
                        }
                    }
                }

                // discard `time` entries with empty `list`.
                stash.retain(|time, list| list.len() &gt; 0);
            }
        });
    });
}
</code></pre></pre>
<p>Take a moment and check out the differences. Mainly, <code>stash</code> is now the one source of truth about <code>time</code> and <code>data</code>, but we now have to do our own checking of <code>time</code> against the input frontiers, and <em>very importantly</em> we need to make sure to discard <code>time</code> from the <code>stash</code> when we are finished with it (otherwise we retain the ability to send at <code>time</code>, and the system will not make progress).</p>
<h1><a class="header" href="#a-worked-example" id="a-worked-example">A Worked Example</a></h1>
<p>You may have heard of <code>word_count</code> as the archetypical &quot;big data&quot; problem: you have a large collection of text, and what you want most in life is to know how many of each word are present in the text. The data are too large to load into memory, but let's assume that the set of distinct words, each with an associated count, is small enough to fit in memory.</p>
<p>Let's take the <code>word_count</code> example in the streaming direction. For whatever reason, your collection of text <em>changes</em>. As time moves along, some new texts are added and some old texts are retracted. We don't know why this happens, we just get told about the changes. Our new job is to <em>maintain</em> the <code>word_count</code> computation, in the face of arbitrary changes to the collection of texts, as promptly as possible.</p>
<p>Let's model a changing corpus of text as a list of pairs of <em>times</em> which will be <code>u64</code> integers with a list of <em>changes</em> which are each pairs <code>(String, i64)</code> indicating the text and whether it has been added (+1) or removed (-1).</p>
<p>We are going to write a program that is the moral equivalent of the following sequential Rust program:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// From a sequence of changes to the occurrences of text,
/// produce the changing counts of words in that text.
fn word_count(mut history: Vec&lt;(u64, Vec&lt;(String, i64)&gt;)&gt;) {
    let mut counts = ::std::collections::HashMap::new();
    for (time, mut changes) in history.drain(..) {
        for (text, diff) in changes.drain(..) {
            for word in text.split_whitespace() {
                let mut entry = counts.entry(word.to_owned())
                                      .or_insert(0i64);
                *entry += diff;
                println!(&quot;seen: {:?}&quot;, (word, *entry));
            }
        }
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>This program is fairly straightforward; hopefully you understand its intent, even if you aren't familiar with every method and type. However, the program is also very specific about what must happen: we process the history in order, and for each time we process the text changes in order. The program does not allow for any flexibility here.</p>
<p>Our program will be a bit larger, but it will be more flexible. By specifying more about what we want to happen to the data, and less about which order this needs to happen, we will gain the ability to scale out to multiple workers across multiple machines.</p>
<h2><a class="header" href="#starting-out-with-text-streams" id="starting-out-with-text-streams">Starting out with text streams</a></h2>
<p>Let's first build a timely computation into which we can send text and which will show us the text back. Our next steps will be to put more clever logic in place, but let's start here to get some boiler-plate out of the way.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::{InputHandle, ProbeHandle};
use timely::dataflow::operators::{Inspect, Probe};

fn main() {
    // initializes and runs a timely dataflow.
    timely::execute_from_args(std::env::args(), |worker| {

        // create input and output handles.
        let mut input = InputHandle::new();
        let mut probe = ProbeHandle::new();

        // build a new dataflow.
        worker.dataflow(|scope| {
            input.to_stream(scope)
                 .inspect(|x| println!(&quot;seen: {:?}&quot;, x))
                 .probe_with(&amp;mut probe);
        });

        // feed the dataflow with data.
        for round in 0..10 {
            input.send((&quot;round&quot;.to_owned(), 1));
            input.advance_to(round + 1);
            while probe.less_than(input.time()) {
                worker.step();
            }
        }
    }).unwrap();
}
</code></pre></pre>
<p>This example code is pretty close to a minimal non-trivial timely dataflow computation. It explains how participating timely workers (there may be many, remember) should construct and run a timely dataflow computation.</p>
<p>After some boiler-plate including the <code>timely</code> crate and some of its traits and types, we get to work:</p>
<pre><code class="language-rust ignore">        // create input and output handles.
        let mut input = InputHandle::new();
        let mut probe = ProbeHandle::new();
</code></pre>
<p>The input handle is how we supply data to the computation, and the probe handle is how we check whether the computation is complete up through certain inputs. Since a streaming computation may never &quot;finish&quot;, <code>probe</code> is the only way to understand how much progress we've made.</p>
<p>The next step is to build a timely dataflow. Here we use <code>input</code> as a source of data, and attach <code>probe</code> to the end so that we can watch for completion of work.</p>
<pre><code class="language-rust ignore">        // build a new dataflow.
        worker.dataflow(|scope| {
            input.to_stream(scope)
                 .inspect(|x| println!(&quot;seen: {:?}&quot;, x))
                 .probe_with(&amp;mut probe);
        });
</code></pre>
<p>This computation is pretty simple: it just prints out the inputs we send at it.</p>
<p>Having constructed the dataflow, we feed it some data.</p>
<pre><code class="language-rust ignore">        // feed the dataflow with data.
        for round in 0..10 {
            input.send((&quot;round&quot;.to_owned(), 1));
            input.advance_to(round + 1);
            while probe.less_than(input.time()) {
                worker.step();
            }
        }
</code></pre>
<p>There are several things going on here. First, we <code>send</code> some data into the input, which allows the data to circulate through the workers along the dataflow. This data will be of type <code>(String, i64)</code>, because our example wants to send some text and annotate each with the change in the count (we add or remove text with <code>+1</code> or <code>-1</code>, respectively). Second, we <code>advance_to</code> to tell timely dataflow that we have ceased sending data for <code>round</code> and anything before it. At this point timely can start to reason about <code>round</code> becoming complete, once all the associated data make their way through the dataflow. Finally, we repeatedly <code>step</code> the worker until <code>probe</code> reports that it has caught up to <code>round + 1</code>, meaning that data for <code>round</code> are fully flushed from the system (and printed to the screen, one hopes).</p>
<h2><a class="header" href="#breaking-text-into-words" id="breaking-text-into-words">Breaking text into words</a></h2>
<p>Let's add a simple operator that takes our text strings we supply as input and breaks them into words.</p>
<p>More specifically, we will take <code>(String, i64)</code> pairs and break them into many <code>(String, i64)</code> pairs with the same <code>i64</code> value, because if we are adding some text we'll add the words, and if subtracting text we'll subtract the words.</p>
<p>Rather than repeat all the code up above, I'm just going to show you the fragment you insert between <code>to_stream</code> and <code>inspect</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">extern crate timely;
</span><span class="boring">
</span><span class="boring">use timely::dataflow::{InputHandle, ProbeHandle};
</span><span class="boring">use timely::dataflow::operators::{Inspect, Probe, Map};
</span><span class="boring">
</span><span class="boring">fn main() {
</span><span class="boring">    // initializes and runs a timely dataflow.
</span><span class="boring">    timely::execute_from_args(std::env::args(), |worker| {
</span><span class="boring">
</span><span class="boring">        // create input and output handles.
</span><span class="boring">        let mut input = InputHandle::new();
</span><span class="boring">        let mut probe = ProbeHandle::new();
</span><span class="boring">
</span><span class="boring">        // build a new dataflow.
</span><span class="boring">        worker.dataflow(|scope| {
</span><span class="boring">            input.to_stream(scope)
</span>                 .flat_map(|(text, diff): (String, i64)|
                     text.split_whitespace()
                         .map(move |word| (word.to_owned(), diff))
                         .collect::&lt;Vec&lt;_&gt;&gt;()
                 )
<span class="boring">                 .inspect(|x| println!(&quot;seen: {:?}&quot;, x))
</span><span class="boring">                 .probe_with(&amp;mut probe);
</span><span class="boring">        });
</span><span class="boring">
</span><span class="boring">        // feed the dataflow with data.
</span><span class="boring">        for round in 0..10 {
</span><span class="boring">            input.send((&quot;round&quot;.to_owned(), 1));
</span><span class="boring">            input.advance_to(round + 1);
</span><span class="boring">            while probe.less_than(input.time()) {
</span><span class="boring">                worker.step();
</span><span class="boring">            }
</span><span class="boring">        }
</span><span class="boring">    }).unwrap();
</span><span class="boring">}
</span></code></pre></pre>
<p>The <code>flat_map</code> method expects to be told how to take each record and turn it into an iterator. Here, we are saying that each received <code>text</code> should be split (at whitespace boundaries), and each resulting <code>word</code> should be paired up with <code>diff</code>. We do a weird <code>collect</code> thing at the end because <code>split_whitespace</code> tries to hand back pointers into <code>text</code> and it makes life complicated. Sorry, blame Rust (and then blame me for using Rust).</p>
<p>This code should now show us the stream of <code>(word, diff)</code> pairs that fly by, but we still haven't done anything complicated with them yet.</p>
<h2><a class="header" href="#maintaining-word-counts" id="maintaining-word-counts">Maintaining word counts</a></h2>
<p>This gets a bit more interesting. We don't have an operator to maintain word counts, so we are going to write one.</p>
<p>We start with a stream of words and differences coming at us. This stream has no particular structure, and in particular if the stream is distributed across multiple workers we have no assurance that all instances of the same word are at the same worker. This means that if each worker just adds up the counts for each word, we will get a bunch of partial results, local to each worker.</p>
<p>We will need to introduce <em>data exchange</em>, where the workers communicate with each other to shuffle the data so that the resulting distribution provides correct results. Specifically, we are going to distribute the data so that each individual word goes to the same worker, but the words themselves may be distributed across workers.</p>
<p>Having exchanged the data, each worker will need a moment of care when it processes its inputs. Because the data are coming in from multiple workers, they may no longer be in &quot;time order&quot;; some workers may have moved through their inputs faster than others, and may be producing data for the next time while others lag behind. This operator means to produce the word count changes <em>as if processed sequentially</em>, and it will need to delay processing changes that come early.</p>
<p>As before, I'm just going to show you the new code, which now lives just after <code>flat_map</code> and just before <code>inspect</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">extern crate timely;
</span><span class="boring">
</span><span class="boring">use timely::dataflow::{InputHandle, ProbeHandle};
</span><span class="boring">use timely::dataflow::operators::{Inspect, Probe, Map};
</span><span class="boring">
</span><span class="boring">use std::collections::HashMap;
</span><span class="boring">use timely::dataflow::channels::pact::Exchange;
</span><span class="boring">use timely::dataflow::operators::Operator;
</span><span class="boring">
</span><span class="boring">
</span><span class="boring">fn main() {
</span><span class="boring">    // initializes and runs a timely dataflow.
</span><span class="boring">    timely::execute_from_args(std::env::args(), |worker| {
</span><span class="boring">
</span><span class="boring">        // create input and output handles.
</span><span class="boring">        let mut input = InputHandle::new();
</span><span class="boring">        let mut probe = ProbeHandle::new();
</span><span class="boring">
</span><span class="boring">        // build a new dataflow.
</span><span class="boring">        worker.dataflow::&lt;usize,_,_&gt;(|scope| {
</span><span class="boring">            input.to_stream(scope)
</span><span class="boring">                .flat_map(|(text, diff): (String, i64)|
</span><span class="boring">                    text.split_whitespace()
</span><span class="boring">                        .map(move |word| (word.to_owned(), diff))
</span><span class="boring">                        .collect::&lt;Vec&lt;_&gt;&gt;()
</span><span class="boring">                )
</span>                .unary_frontier(
                    Exchange::new(|x: &amp;(String, i64)| (x.0).len() as u64),
                    &quot;WordCount&quot;,
                    |_capability, operator_info| {

                    // allocate operator-local storage.
                    let mut queues = HashMap::new();
                    let mut counts = HashMap::new();
                    let mut buffer = Vec::new();

                    move |input, output| {

                        // for each input batch, stash it at `time`.
                        while let Some((time, data)) = input.next() {
                            queues.entry(time.retain())
                                  .or_insert(Vec::new())
                                  .extend(data.replace(Vec::new()));
                        }

                        // enable each stashed time if ready.
                        for (time, vals) in queues.iter_mut() {
                            if !input.frontier().less_equal(time.time()) {
                                let vals = std::mem::replace(vals, Vec::new());
                                buffer.push((time.clone(), vals));
                            }
                        }

                        // drop complete time and allocations.
                        queues.retain(|time, vals| vals.len() &gt; 0);

                        // sort ready updates by time.
                        buffer.sort_by(|x,y| (x.0).time().cmp(&amp;(y.0).time()));

                        // retire updates in time order.
                        for (time, mut vals) in buffer.drain(..) {
                            let mut session = output.session(&amp;time);
                            for (word, diff) in vals.drain(..) {
                                let entry = counts.entry(word.clone()).or_insert(0i64);
                                *entry += diff;
                                session.give((word, *entry));
                            }
                        }
                    }
                })
<span class="boring">                 .inspect(|x| println!(&quot;seen: {:?}&quot;, x))
</span><span class="boring">                 .probe_with(&amp;mut probe);
</span><span class="boring">        });
</span><span class="boring">
</span><span class="boring">        // feed the dataflow with data.
</span><span class="boring">        for round in 0..10 {
</span><span class="boring">            input.send((&quot;round&quot;.to_owned(), 1));
</span><span class="boring">            input.advance_to(round + 1);
</span><span class="boring">            while probe.less_than(input.time()) {
</span><span class="boring">                worker.step();
</span><span class="boring">            }
</span><span class="boring">        }
</span><span class="boring">    }).unwrap();
</span><span class="boring">}
</span></code></pre></pre>
<p>That was probably a lot to see all at once. So let's break down each of the things we did.</p>
<pre><code class="language-rust ignore">.unary_frontier(
    Exchange::new(|x: &amp;(String, i64)| (x.0).len() as u64),
    &quot;WordCount&quot;,
    |_capability, operator_info| {
        // coming soon!
    }
)
</code></pre>
<p>The very first thing we did was state that we are going to build a new unary dataflow operator. Timely lets you build your own operators just by specifying the logic for them as a closure. So easy! But, we have to explain a few things to the operator.</p>
<p>First, we tell it how it should distribute the data (pairs of strings and differences) between workers. Here we are saying &quot;by the length of the text&quot; which is a deranged way to do it, but we'd need about five more lines to properly write hashing code for the string.</p>
<p>Second, we give a descriptive name so that the operator is recognizable in logging and diagnostic code; you probably don't care at the moment, but you might later on if you wonder what is going on.</p>
<p>Third and finally, we specify a closure. The closure has an argument, which we ignore in the code (it has to do with writing operators that can send output data before they receive any input data) and we will ignore it now. This closure is actually a &quot;closure builder&quot;: it is a closure that just returns another closure:</p>
<pre><code class="language-rust ignore">    // allocate operator-local storage.
    let mut queues = HashMap::new();
    let mut counts = HashMap::new();
    let mut buffer = Vec::new();

    move |input, output| {
        // coming soon!
    }
</code></pre>
<p>The closure that we end up returning is the <code>|input, output|</code> closure. It describes what the operator would do when presented with a handle to the input and a handle to the output. We've also named two hash maps and a vector we will need, and provided the <code>move</code> keyword to Rust so that it knows that the resulting closure <em>owns</em> these hash maps, rather than <em>borrows</em> them.</p>
<p>Inside the closure, we do two things: (i) read inputs and (ii) update counts and send outputs. Let's do the input reading first:</p>
<pre><code class="language-rust ignore">        // for each input batch, stash it at `time`.
        while let Some((time, data)) = input.next() {
            queues.entry(time.retain())
                  .or_insert(Vec::new())
                  .extend(data.replace(Vec::new()));
        }
</code></pre>
<p>The <code>input</code> handle has a <code>next</code> method, and it optionally returns a pair of <code>time</code> and <code>data</code>, representing a timely dataflow timestamp and a hunk of data bearing that timestamp, respectively. Our plan is to iterate through all available input (the <code>next()</code> method doesn't block, it just returns <code>None</code> when it runs out of data), accepting it from the timely dataflow system and moving it into our <code>queue</code> hash map.</p>
<p>Why do we do this? Because this is a streaming system, we could be getting data out of order. Our goal is to update the counts in time order, and to do this we'll need to enqueue what we get until we also get word that the associated <code>time</code> is complete. That happens in the next few hunks of code</p>
<p>First, we extract those times and their data that are ready to go:</p>
<pre><code class="language-rust ignore">        // enable each stashed time if ready.
        for (time, vals) in queues.iter_mut() {
            if !input.frontier().less_equal(time.time()) {
                let vals = std::mem::replace(vals, Vec::new());
                buffer.push((time.clone(), vals));
            }
        }
</code></pre>
<p>Here we look through each <code>(time, vals)</code> pair that we've queued up. We then check <code>input.frontier</code>, which is what tells us whether we might expect more times or not. The <code>input.frontier()</code> describes times we may yet see on the input; if it is <code>less_equal</code> to the time, then it is possible there might be more data.</p>
<p>If the time is complete, we extract the data and get ready to act on it. We don't actually act <em>yet</em>, because many times may become available at once, and we want to process them in order too. Before we do that, some housekeeping:</p>
<pre><code class="language-rust ignore">        // drop complete time and allocations.
        queues.retain(|time, vals| vals.len() &gt; 0);

        // sort ready updates by time.
        buffer.sort_by(|x,y| (x.0).time().cmp(&amp;(y.0).time()));
</code></pre>
<p>These calls clean up the <code>queues</code> hash map removing keys we are processing, and then sort <code>buffer</code> by time to make sure we process them in order. This first step is surprisingly important: the keys of this hash map are timestamps that can be used to send data, and we need to drop them for timely dataflow to understand that we give up the ability to send data at these times.</p>
<p>Finally, we drain <code>buffer</code> and process updates in time order</p>
<pre><code class="language-rust ignore">        // retire updates in time order.
        for (time, mut vals) in buffer.drain(..) {
            let mut session = output.session(&amp;time);
            for (word, diff) in vals.drain(..) {
                let entry = counts.entry(word.clone()).or_insert(0i64);
                *entry += diff;
                session.give((word, *entry));
            }
        }
</code></pre>
<p>Here we process each time in order (we sorted them!). For each time, we create a new output session from <code>output</code> using <code>time</code> More importantly, this actually needs to be the same type as <code>time</code> from before; the system is smart and knows that if you drop all references to a time you cannot create new output sessions. It's a feature, not a bug.</p>
<p>We then proceed through each of the batches we enqueue, and through each of the <code>(word, diff)</code> pairs in each of the batches. I've decided that what we are going to do is update the count and announce the new count, but you could probably imagine doing lots of different things here.</p>
<h2><a class="header" href="#the-finished-product" id="the-finished-product">The finished product</a></h2>
<p>You can check out the result in <a href="https://github.com/TimelyDataflow/timely-dataflow/blob/master/timely/examples/wordcount.rs"><code>examples/wordcount.rs</code></a>. If you run it as written, you'll see output that looks like:</p>
<pre><code class="language-ignore">    Echidnatron% cargo run --example wordcount
        Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs
        Running `target/debug/examples/wordcount`
    seen: (&quot;round&quot;, 1)
    seen: (&quot;round&quot;, 2)
    seen: (&quot;round&quot;, 3)
    seen: (&quot;round&quot;, 4)
    seen: (&quot;round&quot;, 5)
    seen: (&quot;round&quot;, 6)
    seen: (&quot;round&quot;, 7)
    seen: (&quot;round&quot;, 8)
    seen: (&quot;round&quot;, 9)
    seen: (&quot;round&quot;, 10)
    Echidnatron%
</code></pre>
<p>We kept sending the same word over and over, so its count went up. Neat. If you'd like to run it with two workers, you just need to put <code>-- -w2</code> at the end of the command, like so:</p>
<pre><code class="language-ignore">    Echidnatron% cargo run --example wordcount -- -w2
        Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs
        Running `target/debug/examples/wordcount -w2`
    seen: (&quot;round&quot;, 1)
    seen: (&quot;round&quot;, 2)
    ...
    seen: (&quot;round&quot;, 19)
    seen: (&quot;round&quot;, 20)
    Echidnatron%
</code></pre>
<p>Because there are two workers, each inputting <code>&quot;round&quot;</code> repeatedly, we count up to twenty. By the end of this text you should be able to produce more interesting examples, for example reading the contents of directories and divvying up responsibility for the files between the workers.</p>
<h1><a class="header" href="#running-timely-dataflows" id="running-timely-dataflows">Running Timely Dataflows</a></h1>
<p>In this section we will look at driving a timely dataflow computation.</p>
<p>With a dataflow graph defined, how do we interactively supply data to the computation, and how do we understand what the computation has actually done given that we are not ourselves doing it? These are good questions, and the dataflow execution model is indeed a bit of a departure from how most folks first experience programming.</p>
<p>The first thing to understand about timely dataflow is that <em>we are programming the worker threads</em>. Part of this program is asking the worker to build up a dataflow graph; we did that when we created an <code>InputHandle</code> and when we called <code>dataflow</code> followed by some <code>filter</code> and <code>map</code> and <code>probe</code> commands. But another part is where we actually start feeding the dataflow graph, advancing the inputs, and letting the worker give each of the operators a chance to run.</p>
<pre><code class="language-rust ignore">for round in 0..10 {
    input.send(round);
    input.advance_to(round + 1);
    while probe.less_than(input.time()) {
        worker.step();
    }
}
</code></pre>
<p>This is the loop that we've seen in several examples. It looks fairly simple, but this is what actually causes work to happen. We do send data and advance the input, but we also call <code>worker.step()</code>, and this is where the actual timely dataflow computation happens. Until you call this, all the data are just building up in queues.</p>
<p>In this section, we'll look at these moving parts in more detail.</p>
<h1><a class="header" href="#providing-input" id="providing-input">Providing Input</a></h1>
<p>The first thing we often see is <code>input.send</code> with some data. This moves the supplied data from the current scope into a queue shared with the input dataflow operator. As this queue starts to fill, perhaps due to you calling <code>send</code> a lot, it moves the data along to its intended recipients. This probably means input queues of other operators, but it may mean serialization and network transmission.</p>
<p>You can call <code>send</code> as much as you like, and the <code>InputHandle</code> implementation will keep moving the data along. The worst that is going to happen is depositing data in shared queues and perhaps some serialization; the call to <code>send</code> will not block, and it should not capture your thread of execution to do any particularly expensive work.</p>
<p>However, since we are writing the worker code, you may want to take a break from <code>send</code> every now and again and let some of the operators run (in just a moment). Part of efficient streaming computation is keeping the data moving, and building up only relatively small buffers before giving the operators a chance to run.</p>
<h2><a class="header" href="#controlling-capabilities" id="controlling-capabilities">Controlling capabilities</a></h2>
<p>The second thing we often see is <code>input.advance_to</code> with a time. This is an exciting moment where the input announces that it will no longer send data timestamped with anything not greater or equal to its argument. This is big news for the rest of the system, as any operator waiting on the timestamp you previously held can now get to work (or, once all the messages you sent have drained, it can get to work).</p>
<p>It is a logic error to call <code>advance_to</code> with a time that is not greater or equal to the current time, which you can read out with <code>input.time</code>. Timely will check this for you and panic if you screw it up. It is a bit like accessing an array out of bounds: you can check ahead of time if you are about to screw up, but you went and did it anyhow.</p>
<p>Finally, you might be interested to call <code>input.close</code>. This method consumes the input and thereby prevents you from sending any more data. This information is <em>very</em> exciting to the system, which can now tell dataflow operators that they won't be hearing much of anything from you any more.</p>
<p><strong>TIP</strong>: It is very important to keep moving your inputs along if you want your dataflow graph to make progress. One of the most common classes of errors is forgetting to advance an <code>InputHandle</code>, and then waiting and waiting and waiting for the cumulative count of records (or whatever) to come out the other end. Timely really wants you to participate and be clear about what you will and will not do in the future.</p>
<p>At the same time, timely's progress tracking does work proportional to the number of timestamps you introduce. If you use a new timestamp for every record, timely will flush its buffers a lot, get very angry with you, and probably fall over. To the extent that you can batch inputs, sending many with the same timestamp, the better.</p>
<h1><a class="header" href="#monitoring-probes" id="monitoring-probes">Monitoring Probes</a></h1>
<p>On the flip side of inputs we have probes. Probes aren't <em>outputs</em> per se, but rather ways for you to monitor progress. They report on the possible timestamps remaining at certain places in the dataflow graph (wherever you attach them).</p>
<p>The easiest way to create a <code>ProbeHandle</code> is by calling <code>.probe()</code> on a stream. This attaches a probe to a point in the dataflow, and when you inspect the probe (in just a moment) you'll learn about what might happen at that point.</p>
<p>You can also create a <code>ProbeHandle</code> directly with <code>ProbeHandle::new()</code>. Such a probe handle is not very interesting yet, but you can attach a probe handle by calling <code>.probe_with(&amp;mut handle)</code> on a stream. This has the cute benefit that you can attach one probe to multiple parts of the dataflow graph, and it will report on the union of their times. If you would like to watch multiple outputs, you could call <code>.probe()</code> multiple times, or attach one common handle to each with multiple calls to <code>.probe_with()</code>. Both are reasonable, depending on whether you need to distinguish between the multiple locations.</p>
<p>A probe handle monitors information that timely provides about the availability of timestamps. You can think of it as holding on to a <code>Vec&lt;Time&gt;</code>, where any possible future time must be greater or equal to one of the elements in the list.</p>
<p>There are a few convenience methods provided, which allow helpful access to the state a probe handle wraps:</p>
<ol>
<li>The <code>less_than(&amp;Time)</code> method returns true if a time strictly less than the argument is possible.</li>
<li>The <code>less_equal(&amp;Time)</code> method returns true if a time less or equal to the argument is possible.</li>
<li>The <code>done()</code> method returns true if no times are possible.</li>
</ol>
<p>Probe handles also have a <code>with_frontier</code> method that allows you to provide a closure that can observe the frontier and return arbitrary results. This is a bit of a song and dance, because the frontier is shared mutable state and cannot be trivially handed back up to your code without peril (you would gain a <code>RefMut</code> that may cause the system to panic if you do not drop before calling <code>worker.step()</code>).</p>
<p>The most common thing to do with a probe handle is to check whether we are &quot;caught up&quot; to the input times. The following is a very safe idiom for doing this:</p>
<pre><code class="language-rust ignore">probe.less_than(input.time())
</code></pre>
<p>This checks if there are any times strictly less than what the input is positioned to provide next. If so, it means we could keep doing work and making progress, because we know that the system <em>could</em> catch up to <code>input.time()</code> as we can't produce anything less than this from <code>input</code>.</p>
<p>However, you are free to use whatever logic you like. Perhaps you just want to check this test a few times, rather than iterating for as long as it is true (which we commonly do). This would give the dataflow a chance to catch up, but it would start work on the next batch of data anyhow, to keep things moving along. There is a trade-off between overloading the system (if you provide data faster than you can retire it) and underloading it by constantly waiting rather than working.</p>
<h1><a class="header" href="#operator-execution" id="operator-execution">Operator Execution</a></h1>
<p>Perhaps the most important statement in a timely dataflow program:</p>
<pre><code class="language-rust ignore">worker.step()
</code></pre>
<p>This is the method that tells the worker that it is now a good time to schedule each of the operators. If you recall, when designing our dataflow we wrote these operators, each of which were programmed by what they would do when shown their input and output handles. This is where we run that code.</p>
<p>The <code>worker.step()</code> call is the heart of data processing in timely dataflow. The system will do a swing through each dataflow operator and call in to its closure once. Each operator has the opportunity to drain its input and produce some output, and depending on how they are coded they may do just that.</p>
<p>Importantly, this is also where we start moving data around. Until we call <code>worker.step()</code> all data are just sitting in queues. The parts of our computation that do clever things like filtering down the data, or projecting out just a few small fields, or pre-aggregating the data before we act on it, these all happen here and don't happen until we call this.</p>
<p>Make sure to call <code>worker.step()</code> now and again, like you would your parents.</p>
<h1><a class="header" href="#extending-dataflows" id="extending-dataflows">Extending Dataflows</a></h1>
<p>This might be surprising to see in the &quot;Running Timely Dataflows&quot; section, but it is worth pointing out. Just because we built one dataflow doesn't mean that we have to stop there. We can run one dataflow for a while, and then create a second dataflow and run it. While we are running those (just one <code>worker.step()</code> calls into both) we could create a third and start running that too.</p>
<p>The <code>worker</code> can track an arbitrary number of dataflows, and will clean up after each of them once it is complete (when no capabilities exist in the dataflow). You are welcome to spin up as many as you like, if there are reasons you might need several in the course of one program.</p>
<p>You can also do something fun that we're working on (in progress), which is to map in shared libraries and load the dataflows they define. This gives rise to something like a &quot;timely dataflow cluster&quot; that can accept jobs (in the form of shared libraries) which are installed and run sharing the resources with other dataflows. Of course, if they crash they take down everything, so bear this in mind before getting too excited.</p>
<h1><a class="header" href="#advanced-timely-dataflow" id="advanced-timely-dataflow">Advanced Timely Dataflow</a></h1>
<p>In this section we will cover some of the more advanced topics in timely dataflow. Most of these topics derive from existing concepts, but build up the complexity a level.</p>
<p>Much of what we'll learn about here looks like <em>control flow</em>: dataflow patterns that cause our computation to execute in interesting and new ways, other than a straight line of dataflow. These patterns connect back to idioms from sequential imperative data processing, but using progress information to resculpt the movement of data, changing what computation actually occurs.</p>
<h1><a class="header" href="#scopes" id="scopes">Scopes</a></h1>
<p>The first bit of complexity we will introduce is the timely dataflow <em>scope</em>. A scope is a region of dataflow, much like how curly braces provide scopes in imperative languages:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>{
    // this is an outer scope

    {
        // this is an inner scope
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>You can create a new scope in any other scope by invoking the <code>scoped</code> method:</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::Scope;

fn main() {
    timely::example(|scope| {

        // Create a new scope with the same (u64) timestamp.
        scope.scoped::&lt;u64,_,_&gt;(&quot;SubScope&quot;, |subscope| {
            // probably want something here
        })

    });
}
</code></pre></pre>
<p>The main thing that a scope does for you is allow you to enrich the timestamp of all streams that are brought into it. For example, perhaps the timestamp type in your base dataflow scope is <code>usize</code>, your nested scope could augment this to <code>(usize, u32)</code>, or whatever additional timestamp type you might want. In the example above, we choose to keep the same timestamp, <code>u64</code>.</p>
<p>Why are additional timestamps useful? They allow operators in the nested scope to change their behavior based on the enriched timestamp, without worrying the streams and operators outside the scope about this detail. We will soon see the example of <em>iterative</em> dataflow, where a scope allows cycles within it, but does not bother operators outside the scope with that detail.</p>
<h2><a class="header" href="#entering-and-exiting-scopes" id="entering-and-exiting-scopes">Entering and exiting scopes</a></h2>
<p>In addition to <em>creating</em> scopes, we will also need to get streams of data into and out of scopes.</p>
<p>There are two simple methods, <code>enter</code> and <code>leave</code>, that allow streams of data into and out of scopes. It is important that you use them! If you try to use a stream in a nested scope, Rust will be confused because it can't get the timestamps of your streams to typecheck.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::Scope;
use timely::dataflow::operators::*;

fn main() {
    timely::example(|scope| {

        let stream = (0 .. 10).to_stream(scope);

        // Create a new scope with the same (u64) timestamp.
        let result = scope.scoped::&lt;u64,_,_&gt;(&quot;SubScope&quot;, |subscope| {
            stream.enter(subscope)
                  .inspect_batch(|t, xs| println!(&quot;{:?}, {:?}&quot;, t, xs))
                  .leave()
        });

    });
}
</code></pre></pre>
<p>Notice how we can both <code>enter</code> a stream and <code>leave</code> in a single sequence of operations.</p>
<p>The <code>enter</code> operator introduces each batch of records as a batch with an enriched timestamp, which usually means &quot;the same&quot; or &quot;with a new zero coordinate&quot;. The <code>leave</code> just de-enriches the timestamp, correspondingly &quot;the same&quot; or &quot;without that new coordinate&quot;. The <code>leave</code> operator results in a stream fit for consumption in the containing scope.</p>
<h2><a class="header" href="#regions" id="regions">Regions</a></h2>
<p>There is a handy shortcut for scopes that do not change the timestamp type, a <code>region</code>.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::Scope;
use timely::dataflow::operators::*;

fn main() {
    timely::example(|scope| {

        let stream = (0 .. 10).to_stream(scope);

        // Create a new scope with the same (u64) timestamp.
        let result = scope.region(|subscope| {
            stream.enter(subscope)
                  .inspect_batch(|t, xs| println!(&quot;{:?}, {:?}&quot;, t, xs))
                  .leave()
        });

    });
}
</code></pre></pre>
<p>Regions are mostly here to help you organize your dataflow. A region presents itself to its surrounding dataflow as a single operator, and that can make it easier for you or others to reason about the logical structure of your dataflow.</p>
<p><strong>IMPORTANT</strong> Although you can probably write a program that uses <code>region()</code> but then skips the calls to <code>enter</code> and <code>leave</code>, because the timestamp types are the same, this is very naughty and please do not do this.</p>
<h2><a class="header" href="#iteration" id="iteration">Iteration</a></h2>
<p>Iteration is a particularly neat form of nested scope in which all timestamps are enriched with an iteration counter. This counter starts at zero for streams that <code>enter</code> the scope, may increment in the context of the loop (next section gives examples of this), and is removed when the <code>leave</code> method is called.</p>
<p>The enriched timestamp type is <code>timely::order::Product&lt;TOuter, TInner&gt;</code>, for an outer timestamp type <code>TOuter</code> and an iteration counter type <code>TInner</code>. However, there is a convenience method that can allow you to skip thinking too hard about this.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::Scope;
use timely::dataflow::operators::*;

fn main() {
    timely::example(|scope| {

        let stream = (0 .. 10).to_stream(scope);

        // Create a new scope with a (u64, u32) timestamp.
        let result = scope.iterative::&lt;u32,_,_&gt;(|subscope| {
            stream.enter(subscope)
                  .inspect_batch(|t, xs| println!(&quot;{:?}, {:?}&quot;, t, xs))
                  .leave()
        });

    });
}
</code></pre></pre>
<h1><a class="header" href="#iteration-1" id="iteration-1">Iteration</a></h1>
<p>One of the neatest things you can do in timely dataflow is iteration. It is a consequence of how timely dataflow tracks progress that we can build loops, circulate data within the loop, and still allow downstream operators to properly judge the progress of their input streams.</p>
<p>There is nothing special about iterative scopes. They are just timely dataflow scopes. In fact, we could even introduce cycles into the root scope that all <code>dataflow</code> calls provide.</p>
<p>What timely dataflow provides is a special stream called a <code>LoopVariable</code>. This is a stream whose contents are not yet defined. You can start using the stream as if they were, but they won't be until later on in the dataflow when we define them.</p>
<p>That may be a bit abstract, so let's look at a simple example.</p>
<p>We are going to check the <a href="https://en.wikipedia.org/wiki/Collatz_conjecture">Collatz conjecture</a>, which says that if you repeatedly divide even numbers by two, and multiply odd numbers by three and add one, you eventually reach the number one. We could do this in lots of ways, but this is the timely dataflow way to do it.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::*;

fn main() {
    timely::example(|scope| {

        // create a loop that cycles unboundedly.
        let (handle, stream) = scope.feedback(1);

        // circulate numbers, Collatz stepping each time.
        (1 .. 10)
            .to_stream(scope)
            .concat(&amp;stream)
            .map(|x| if x % 2 == 0 { x / 2 } else { 3 * x + 1 } )
            .inspect(|x| println!(&quot;{:?}&quot;, x))
            .filter(|x| *x != 1)
            .branch_when(|t| t &lt; &amp;100).1
            .connect_loop(handle);
    });
}
</code></pre></pre>
<p>This program first creates a loop variable, using the <code>feedback</code> method on scopes. This method comes from the <code>Feedback</code> extension trait in <code>dataflow::operators</code>, in case you can't find it. When we create a new loop variable, we have to tell timely dataflow by how much we should increment the timestamp each time around the loop. To be more specific, we have to give a path summary which often is just a number that tells us by how much to increment the timestamp. When we later connect the output of an operation back to this loop variable we can specify an upper bound on the number of iterations by using the <code>branch_when</code> method.</p>
<p>We start with a stream of the numbers from one through nine, because we have to start somewhere. Our plan is to repeatedly apply the Collatz step, and then discard any numbers equal to one, but we want to apply this not only to our input but also to whatever comes back around our loop variable. So, the very first step is to <code>concat</code> our input stream with the feedback stream. Then we can apply the Collatz step, filter out the ones, and then connect the resulting stream as the definition of the feedback stream.</p>
<p>We've built an upper limit of <code>100</code> in so that we don't spin out of control, in case the conjecture is false. It turns out that <code>9</code> will take 19 steps to converge, so this should be good enough. You could try it out for larger numbers!</p>
<h2><a class="header" href="#mutual-recursion" id="mutual-recursion">Mutual Recursion</a></h2>
<p>You can have as many loop variables as you want!</p>
<p>Perhaps you are a very clever person, and you've realized that we don't need to test the results of odd numbers to see if they are one, which would be easy if we broke our stream apart into streams of even and odd numbers. We can do that, by having two loop variables, one for even (<code>0</code>) and one for odd (<code>1</code>).</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::*;

fn main() {
    timely::example(|scope| {

        // create a loop that cycles unboundedly.
        let (handle0, stream0) = scope.feedback(1);
        let (handle1, stream1) = scope.feedback(1);

        // do the right steps for even and odd numbers, respectively.
        let results0 = stream0.map(|x| x / 2).filter(|x| *x != 1);
        let results1 = stream1.map(|x| 3 * x + 1);

        // partition the input and feedback streams by even-ness.
        let parts =
            (1 .. 10)
                .to_stream(scope)
                .concat(&amp;results0)
                .concat(&amp;results1)
                .inspect(|x| println!(&quot;{:?}&quot;, x))
                .partition(2, |x| (x % 2, x));

        // connect each part appropriately.
        parts[0].connect_loop(handle0);
        parts[1].connect_loop(handle1);
    });
}
</code></pre></pre>
<p>This is a different way to do the same computation (similar; because we've moved input from the head of the dataflow, the limit of <code>100</code> is effectively reduced by one). I won't say it is a better way to do the same computation, but it is different.</p>
<h2><a class="header" href="#scopes-1" id="scopes-1">Scopes</a></h2>
<p>Of course, you can do all of this in a nested scope, if that is appropriate. In the example above, we just used the ambient <code>u64</code> timestamp from <code>timely::example</code>, but perhaps you were hoping that it would correspond to streamed input, or something else. We just need to introduce a new nested scope, in that case.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::*;
use timely::dataflow::Scope;

fn main() {
    timely::example(|scope| {

        let input = (1 .. 10).to_stream(scope);

        // Create a nested iterative scope.
        // Rust needs help understanding the iteration counter type.
        scope.iterative::&lt;u64,_,_&gt;(|subscope| {

            let (handle, stream) = subscope.loop_variable(1);

            input
                .enter(subscope)
                .concat(&amp;stream)
                .map(|x| if x % 2 == 0 { x / 2 } else { 3 * x + 1 } )
                .inspect(|x| println!(&quot;{:?}&quot;, x))
                .filter(|x| *x != 1)
                .connect_loop(handle);
        })

    });
}
</code></pre></pre>
<p>We could also pop out the results with <code>leave</code> if we had any.</p>
<p><strong>Exercise</strong>: Rewrite the above example so that while iterating each tuple tracks (i) its progenitor number, and (ii) the number of times it has gone around the loop, and rather than discarding tuples whose value is now one they <code>leave</code> the loop and report for each input number how many steps it takes to reach one.</p>
<h1><a class="header" href="#flow-control" id="flow-control">Flow Control</a></h1>
<p><strong>IN PROGRESS</strong></p>
<p>Data flow along dataflow graphs. It is what they do. It says it in the name. But sometimes we want to control how the data in the dataflow flow. Not <em>where</em> the data flow, we already have controls for doing that (<code>exchange</code>, <code>partition</code>), but rather <em>when</em> the data flow.</p>
<p>Let's consider a simple example, where we take an input stream of numbers, and produce all numbers less than each input.</p>
<pre><pre class="playground"><code class="language-rust">extern crate timely;

use timely::dataflow::operators::*;

fn main() {
    timely::example(|scope| {

        // Produce all numbers less than each input number.
        (1 .. 10)
            .to_stream(scope)
            .flat_map(|x| (0 .. x));

    });
}
</code></pre></pre>
<p>Each number that you put into this dataflow (those <code>0 .. 10</code> folks) can produce a larger number of output records. Let's say you put <code>1000 .. 1010</code> as input instead? We'd have ten thousand records flowing around. What about <code>1000 .. 2000</code>?</p>
<p>This dataflow can greatly increase the amount of data moving around. We might have thought &quot;let's process just 1000 records&quot;, but this turned into one million records. Perhaps we have a few of these operators in a row (don't ask; it happens), we can pretty quickly overwhelm the system if we aren't careful.</p>
<p>In most systems this is mitigated by <em>flow control</em>, mechanisms that push back when it seems like operators are producing more data than can be consumed in the same amount of time.</p>
<p>Timely dataflow doesn't have a built in notion of flow control. Sometimes you want it, sometimes you don't, so we didn't make you have it. Also, it is hard to get right, for similar reasons. Instead, timely dataflow scopes can be used for application-level flow control.</p>
<p>Let's take a simple example, where we have a stream of timestamped numbers coming at us, performing the <code>flat_map</code> up above. Our goal is to process all of the data, but to do so in a controlled manner where we never overwhelm the computation. For example, we might want to do approximately this:</p>
<pre><pre class="playground"><code class="language-rust no_run">extern crate timely;

use timely::dataflow::operators::*;

fn main() {
    timely::example(|scope| {

        // Produce all numbers less than each input number.
        (1 .. 100_000)
            .to_stream(scope)
            .flat_map(|x| (0 .. x));

    });
}
</code></pre></pre>
<p>but without actually producing the 4,999,950,000 intermediate records all at once.</p>
<p>One way to do this is to build a self-regulating dataflow, into which we can immediately dump all the records, but which will buffer records until it is certain that the work for prior records has drained. We will write this out in all the gory details, but these operators are certainly things that could be packaged up and reused.</p>
<p>The idea here is to take our stream of work, and to use the <code>delay</code> operator to assign new timestamps to the records. We will spread the work out so that each timestamp has at most (in this case) 100 numbers. We can write a <code>binary</code> operator that will buffer received records until their timestamp is &quot;next&quot;, meaning all strictly prior work has drained from the dataflow fragment. How do we do this? We turn our previously unary operator into a binary operator that has a feedback edge connected to its second input. We use the frontier of that feedback input to control when we emit data.</p>
<pre><pre class="playground"><code class="language-rust no_run">extern crate timely;

use timely::dataflow::operators::*;
use timely::dataflow::channels::pact::Pipeline;

fn main() {
    timely::example(|scope| {

        let mut stash = ::std::collections::HashMap::new();

        // Feedback loop for noticing progress.
        let (handle, cycle) = scope.feedback(1);

        // Produce all numbers less than each input number.
        (1 .. 100_000u64)
            .to_stream(scope)
            // Assign timestamps to records so that not much work is in each time.
            .delay(|number, time| number / 100 )
            // Buffer records until all prior timestamps have completed.
            .binary_frontier(&amp;cycle, Pipeline, Pipeline, &quot;Buffer&quot;, move |capability, info| {

                let mut vector = Vec::new();

                move |input1, input2, output| {

                    // Stash received data.
                    input1.for_each(|time, data| {
                        data.swap(&amp;mut vector);
                        stash.entry(time.retain())
                             .or_insert(Vec::new())
                             .extend(vector.drain(..));
                    });

                    // Consider sending stashed data.
                    for (time, data) in stash.iter_mut() {
                        // Only send data once the probe is not less than the time.
                        // That is, once we have finished all strictly prior work.
                        if !input2.frontier().less_than(time.time()) {
                            output.session(&amp;time).give_iterator(data.drain(..));
                        }
                    }

                    // discard used capabilities.
                    stash.retain(|_time, data| !data.is_empty());
                }
            })
            .flat_map(|x| (0 .. x))
            // Discard data and connect back as an input.
            .filter(|_| false)
            .connect_loop(handle);
    });
}
</code></pre></pre>
<p>This version of the code tops out at about 64MB on my laptop and takes 45 seconds, whereas the version with <code>unary</code> commented out heads north of 32GB before closing out after two minutes.</p>
<h1><a class="header" href="#capture-and-replay" id="capture-and-replay">Capture and Replay</a></h1>
<p>Timely dataflow has two fairly handy operators, <code>capture_into</code> and <code>replay_from</code>, that are great for transporting a timely dataflow stream from its native representation into data, and then back again. They are also a fine way to think about interoperating with other systems for streaming data.</p>
<h2><a class="header" href="#capturing-streams-1" id="capturing-streams-1">Capturing Streams</a></h2>
<p>At its core, <code>capture_into</code> records everything it sees about the stream it is attached to. If some data arrive, it records that. If there is a change in the possibility that timestamps might arrive on its input, it records that.</p>
<p>The <code>capture_into</code> method is relative simple, and we can just look at it:</p>
<pre><code class="language-rust ignore">    fn capture_into&lt;P: EventPusher&lt;S::Timestamp, D&gt;+'static&gt;(&amp;self, event_pusher: P) {

        let mut builder = OperatorBuilder::new(&quot;Capture&quot;.to_owned(), self.scope());
        let mut input = PullCounter::new(builder.new_input(self, Pipeline));
        let mut started = false;

        let event_pusher1 = Rc::new(RefCell::new(event_pusher));
        let event_pusher2 = event_pusher1.clone();

        builder.build(
            move |frontier| {
                if !started {
                    frontier[0].update(Default::default(), -1);
                    started = true;
                }
                if !frontier[0].is_empty() {
                    let to_send = ::std::mem::replace(&amp;mut frontier[0], ChangeBatch::new());
                    event_pusher1.borrow_mut().push(Event::Progress(to_send.into_inner()));
                }
            },
            move |consumed, _internal, _external| {
                let mut borrow = event_pusher2.borrow_mut();
                while let Some((time, data)) = input.next() {
                    borrow.push(Event::Messages(time.clone(), data.deref_mut().clone()));
                }
                input.consumed().borrow_mut().drain_into(&amp;mut consumed[0]);
                false
            }
        );
    }
</code></pre>
<p>The method is generic with respect to some implementor <code>P</code> of the trait <code>EventPusher</code> which defines a method <code>push</code> that accepts <code>Event&lt;T, D&gt;</code> items (we will see a few implementations in just a moment). After a bit of set-up, <code>capture_into</code> builds a new operator with one input and zero outputs, and sets the logic for (i) what to do when the input frontier changes, and (ii) what to do when presented with the opportunity to do a bit of computation. In both cases, we just create new events based on what we see (progress changes and data messages, respectively).</p>
<p>There is a mysterious subtraction of <code>Default::default()</code>, which has to do with the contract that the replaying operators assume the stream starts with such a capability. This prevents the need for the replayers to block on the stream in their operator construction (any operator must state any initial capabilities as part of its construction; it cannot defer that until later).</p>
<p>One nice aspect of <code>capture_into</code> is that it really does reveal everything that an operator sees about a stream. If you got your hands on the resulting sequence of events, you would be able to review the full history of the stream. In principle, this could be a fine place to persist the data, capturing both data and progress information.</p>
<h2><a class="header" href="#replaying-streams" id="replaying-streams">Replaying Streams</a></h2>
<p>At <em>its</em> core, <code>replay_into</code> takes some sequence of <code>Event&lt;T, D&gt;</code> items and reproduces the stream, as it was recorded. It is also fairly simple, and we can just look at its implementation as well:</p>
<pre><code class="language-rust ignore">    fn replay_into&lt;S: Scope&lt;Timestamp=T&gt;&gt;(self, scope: &amp;mut S) -&gt; Stream&lt;S, D&gt;{

        let mut builder = OperatorBuilder::new(&quot;Replay&quot;.to_owned(), scope.clone());
        let (targets, stream) = builder.new_output();

        let mut output = PushBuffer::new(PushCounter::new(targets));
        let mut event_streams = self.into_iter().collect::&lt;Vec&lt;_&gt;&gt;();
        let mut started = false;

        builder.build(
            move |_frontier| { },
            move |_consumed, internal, produced| {

                if !started {
                    internal[0].update(Default::default(), (event_streams.len() as i64) - 1);
                    started = true;
                }

                for event_stream in event_streams.iter_mut() {
                    while let Some(event) = event_stream.next() {
                        match *event {
                            Event::Start =&gt; { },
                            Event::Progress(ref vec) =&gt; {
                                internal[0].extend(vec.iter().cloned());
                            },
                            Event::Messages(ref time, ref data) =&gt; {
                                output.session(time).give_iterator(data.iter().cloned());
                            }
                        }
                    }
                }

                output.cease();
                output.inner().produced().borrow_mut().drain_into(&amp;mut produced[0]);

                false
            }
        );

        stream
   }
</code></pre>
<p>The type of <code>self</code> here is actually something that allows us to enumerate a sequence of event streams, so each replayer is actually replaying some variable number of streams. As part of this, our very first action is to amend our initial <code>Default::default()</code> capability to have multiplicity equal to the number of streams we are replaying:</p>
<pre><code class="language-rust ignore">                if !started {
                    internal[0].update(Default::default(), (event_streams.len() as i64) - 1);
                    started = true;
                }
</code></pre>
<p>If we have multiple streams, we'll now have multiple capabilities. If we have no stream, we will just drop the capability. This change is important because each source stream believes it has such a capability, and we will eventually see this many drops of the capability in the event stream (though perhaps not immediately; the initial deletion we inserted in <code>capture_into</code> likely cancels with the initial capabilities expressed by the outside world; we will likely need to wait until the captured stream is informed about the completion of messages with the default time).</p>
<p>Having done the initial adjustment, we literally just play out the streams (note the plural) as they are available. The <code>next</code> method is expected not to block, but rather to return <code>None</code> when there are no more events currently available. It is a bit of a head-scratcher, but any interleaving of these streams is itself a valid stream (messages are sent and capabilities claimed only when we hold appropriate capabilities).</p>
<h2><a class="header" href="#an-example-3" id="an-example-3">An Example</a></h2>
<p>We can check out the examples <code>examples/capture_send.rs</code> and <code>examples/capture_recv.rs</code> to see a paired use of capture and receive demonstrating the generality.</p>
<p>The <code>capture_send</code> example creates a new TCP connection for each worker, which it wraps and uses as an <code>EventPusher</code>. Timely dataflow takes care of all the serialization and stuff like that (warning: it uses abomonation, so this is not great for long-term storage).</p>
<pre><code class="language-rust ignore">extern crate timely;

use std::net::TcpStream;
use timely::dataflow::operators::ToStream;
use timely::dataflow::operators::capture::{Capture, EventWriter};

fn main() {
    timely::execute_from_args(std::env::args(), |worker| {

        let addr = format!(&quot;127.0.0.1:{}&quot;, 8000 + worker.index());
        let send = TcpStream::connect(addr).unwrap();

        worker.dataflow::&lt;u64,_,_&gt;(|scope|
            (0..10u64)
                .to_stream(scope)
                .capture_into(EventWriter::new(send))
        );
    }).unwrap();
}
</code></pre>
<p>The <code>capture_recv</code> example is more complicated, because we may have a different number of workers replaying the stream than initially captured it.</p>
<pre><code class="language-rust ignore">extern crate timely;

use std::net::TcpListener;
use timely::dataflow::operators::Inspect;
use timely::dataflow::operators::capture::{EventReader, Replay};

fn main() {
    timely::execute_from_args(std::env::args(), |worker| {

        let source_peers = std::env::args().nth(1).unwrap().parse::&lt;usize&gt;().unwrap();

        // create replayers from disjoint partition of source worker identifiers.
        let replayers =
        (0 .. source_peers)
            .filter(|i| i % worker.peers() == worker.index())
            .map(|i| TcpListener::bind(format!(&quot;127.0.0.1:{}&quot;, 8000 + i)).unwrap())
            .collect::&lt;Vec&lt;_&gt;&gt;()
            .into_iter()
            .map(|l| l.incoming().next().unwrap().unwrap())
            .map(|r| EventReader::&lt;_,u64,_&gt;::new(r))
            .collect::&lt;Vec&lt;_&gt;&gt;();

        worker.dataflow::&lt;u64,_,_&gt;(|scope| {
            replayers
                .replay_into(scope)
                .inspect(|x| println!(&quot;replayed: {:?}&quot;, x));
        })
    }).unwrap(); // asserts error-free execution
}
</code></pre>
<p>Almost all of the code up above is assigning responsibility for the replaying between the workers we have (from <code>worker.peers()</code>). We partition responsibility for <code>0 .. source_peers</code> among the workers, create <code>TcpListener</code>s to handle the connection requests, wrap them in <code>EventReader</code>s, and then collect them up as a vector. The workers have collectively partitioned the incoming captured streams between themselves.</p>
<p>Finally, each worker just uses the list of <code>EventReader</code>s as the argument to <code>replay_into</code>, and we get the stream magically transported into a new dataflow, in a different process, with a potentially different number of workers.</p>
<p>If you want to try it out, make sure to start up the <code>capture_recv</code> example first (otherwise the connections will be refused for <code>capture_send</code>) and specify the expected number of source workers, modifying the number of received workers if you like. Here we are expecting five source workers, and distributing them among three receive workers (to make life complicated):</p>
<pre><code class="language-ignore">    shell1% cargo run --example capture_recv -- 5 -w3
</code></pre>
<p>Nothing happens yet, so head over to another shell and run <code>capture_send</code> with the specified number of workers (five, in this case):</p>
<pre><code class="language-ignore">    shell2% cargo run --example capture_send -- -w5
</code></pre>
<p>Now, back in your other shell you should see something like</p>
<pre><code class="language-ignore">    shell1% cargo run --example capture_recv -- 5 -w3
    replayed: 0
    replayed: 1
    replayed: 2
    replayed: 3
    replayed: 4
    replayed: 5
    replayed: 0
    replayed: 6
    replayed: 1
    ...
</code></pre>
<p>which just goes on and on, but which should produce 50 lines of text, with five copies of <code>0 .. 10</code> interleaved variously.</p>
<h2><a class="header" href="#capture-types" id="capture-types">Capture types</a></h2>
<p>There are several sorts of things you could capture into and replay from. In the <code>capture::events</code> module you will find two examples, a linked list and a binary serializer / deserializer (wrapper around <code>Write</code> and <code>Read</code> traits). The binary serializer is fairly general; we used it up above to wrap TCP streams. You could also write to files, or write to shared memory. However, be mindful that the serialization format (abomonation) is essentially the in-memory representation, and Rust makes no guarantees about the stability of such a representation across builds.</p>
<p>There is also <a href="https://github.com/frankmcsherry/timely-dataflow/tree/master/kafkaesque">an in-progress Kafka adapter</a> available in the repository, which uses Kafka topics to store the binary representation of captured streams, which can then be replayed by any timely computation that can read them. This may be a while before it is sorted out, because Kafka seems to have a few quirks, but if you would like to help get in touch.</p>
<h1><a class="header" href="#custom-datatypes" id="custom-datatypes">Custom Datatypes</a></h1>
<p><strong>WORK IN PROGRESS</strong></p>
<p>Timely dataflow allows you to use a variety of Rust types, but you may also find that you need (or would prefer) your own <code>struct</code> and <code>enum</code> types.</p>
<p>Timely dataflow provides two traits, <code>Data</code> and <code>ExchangeData</code> for types that timely dataflow can transport within a worker thread and across threads.</p>
<h2><a class="header" href="#the-data-trait" id="the-data-trait">The <code>Data</code> trait</a></h2>
<p>The <code>Data</code> trait is essentially a synonym for <code>Clone+'static</code>, meaning the type must be cloneable and cannot contain any references with other than a static lifetime. Most types implement these traits automatically, but if yours do not you should decorate your struct definition with a derivation of the <code>Clone</code> trait:</p>
<pre><code class="language-rust ignore">#[derive(Clone)]
struct YourStruct { .. }
</code></pre>
<h2><a class="header" href="#the-exchangedata-trait" id="the-exchangedata-trait">The <code>ExchangeData</code> trait</a></h2>
<p>The <code>ExchangeData</code> trait is more complicated, and is established in the <code>communication/</code> module. There are two options for this trait, which are determined by whether you use the <code>--bincode</code> feature at compilation, or not.</p>
<ul>
<li>
<p>If you use <code>--bincode</code> then the trait is a synonym for</p>
<pre><code class="language-rust ignore">Send+Sync+Any+serde::Serialize+for&lt;'a&gt;serde::Deserialize&lt;'a&gt;+'static
</code></pre>
<p>where <code>serde</code> is Rust's most popular serialization and deserialization crate. A great many types implement these traits. If your types does not, you should add these decorators to their definition:</p>
<pre><code class="language-rust ignore">#[derive(Serialize, Deserialize)]
</code></pre>
<p>You must include the <code>serde</code> crate, and if not on Rust 2018 the <code>serde_derive</code> crate.</p>
<p>The downside to the <code>--bincode</code> flag is that deserialization will always involve a clone of the data, which has the potential to adversely impact performance. For example, if you have structures that contain lots of strings, timely dataflow will create allocations for each string even if you do not plan to use all of them.</p>
</li>
<li>
<p>If you do not use the <code>--bincode</code> feature, then the <code>Serialize</code> and <code>Deserialize</code> requirements are replaced by <code>Abomonation</code>, from the <code>abomonation</code> crate. This trait allows in-place deserialization, but is implemented for fewer types, and has the potential to be a bit scarier (due to in-place pointer correction).</p>
<p>Your types likely do not implement <code>Abomonation</code> by default, but you can similarly use</p>
<pre><code class="language-rust ignore">#[derive(Abomonation)]
</code></pre>
<p>You must include the <code>abomonation</code> and <code>abomonation_derive</code> crate for this to work correctly.</p>
</li>
</ul>
<h2><a class="header" href="#an-example-4" id="an-example-4">An example</a></h2>
<p>Let's imagine you would like to play around with a tree data structure as something you might send around in timely dataflow. I've written the following candidate example:</p>
<pre><code class="language-rust ignore">extern crate timely;

use timely::dataflow::operators::*;

fn main() {
    timely::example(|scope| {
        (0..10).to_stream(scope)
               .map(|x| TreeNode::new(x))
               .inspect(|x| println!(&quot;seen: {:?}&quot;, x));
    });
}

// A tree structure.
struct TreeNode&lt;D&gt; {
    data: D,
    children: Vec&lt;TreeNode&lt;D&gt;&gt;,
}

impl&lt;D&gt; TreeNode&lt;D&gt; {
    fn new(data: D) -&gt; Self {
        Self { data, children: Vec::new() }
    }
}
</code></pre>
<p>This doesn't work. You'll probably get two errors, that <code>TreeNode</code> doesn't implement <code>Clone</code>, nor does it implement <code>Debug</code>. Timely data types need to implement <code>Clone</code>, and our attempt to print out the trees requires an implementation of <code>Debug</code>. We can create these implementations by decorating the <code>struct</code> declaration like so:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Clone, Debug)]
struct TreeNode&lt;D&gt; {
    data: D,
    children: Vec&lt;TreeNode&lt;D&gt;&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<p>This works for me! We see</p>
<pre><code class="language-ignore">    Echidnatron% cargo run --example types
       Compiling timely v0.8.0 (/Users/mcsherry/Projects/timely-dataflow)
        Finished dev [unoptimized + debuginfo] target(s) in 5.33s
         Running `target/debug/examples/types`
    seen: TreeNode { data: 0, children: [] }
    seen: TreeNode { data: 1, children: [] }
    seen: TreeNode { data: 2, children: [] }
    seen: TreeNode { data: 3, children: [] }
    seen: TreeNode { data: 4, children: [] }
    seen: TreeNode { data: 5, children: [] }
    seen: TreeNode { data: 6, children: [] }
    seen: TreeNode { data: 7, children: [] }
    seen: TreeNode { data: 8, children: [] }
    seen: TreeNode { data: 9, children: [] }
    Echidnatron%
</code></pre>
<h3><a class="header" href="#exchanging-data" id="exchanging-data">Exchanging data</a></h3>
<p>Let's up the level a bit and try and shuffle our tree data between workers.</p>
<p>If we replace our <code>main</code> method with this new one:</p>
<pre><code class="language-rust ignore">extern crate timely;

use timely::dataflow::operators::*;

fn main() {
    timely::example(|scope| {
        (0..10).to_stream(scope)
               .map(|x| TreeNode::new(x))
               .exchange(|x| x.data)
               .inspect(|x| println!(&quot;seen: {:?}&quot;, x));
    });
}

#[derive(Clone, Debug)]
struct TreeNode&lt;D&gt; {
    data: D,
    children: Vec&lt;TreeNode&lt;D&gt;&gt;,
}

impl&lt;D&gt; TreeNode&lt;D&gt; {
    fn new(data: D) -&gt; Self {
        Self { data, children: Vec::new() }
    }
}
</code></pre>
<p>We get a new error. A not especially helpful error. It says that it cannot find an <code>exchange</code> method, or more specifically that one exists but it doesn't apply to our type at hand. This is because the data need to satisfy the <code>ExchangeData</code> trait but do not. It would be better if this were clearer in the error messages, I agree.</p>
<p>We can fix the problem two ways. First, if you would like to use <code>bincode</code>, then we update the source like so:</p>
<pre><code class="language-rust ignore">#[macro_use]
extern crate serde_derive;
extern crate serde;

#[derive(Clone, Debug, Serialize, Deserialize)]
struct TreeNode&lt;D&gt; {
    data: D,
    children: Vec&lt;TreeNode&lt;D&gt;&gt;,
}
</code></pre>
<p>and make sure to include the <code>serde_derive</code> and <code>serde</code> crates. Now when we run things (notice the <code>--features</code> flag) we see:</p>
<pre><code class="language-ignore">    Echidnatron% cargo run --example types --features bincode
        Finished dev [unoptimized + debuginfo] target(s) in 0.07s
         Running `target/debug/examples/types`
    seen: TreeNode { data: 0, children: [] }
    seen: TreeNode { data: 1, children: [] }
    seen: TreeNode { data: 2, children: [] }
    seen: TreeNode { data: 3, children: [] }
    seen: TreeNode { data: 4, children: [] }
    seen: TreeNode { data: 5, children: [] }
    seen: TreeNode { data: 6, children: [] }
    seen: TreeNode { data: 7, children: [] }
    seen: TreeNode { data: 8, children: [] }
    seen: TreeNode { data: 9, children: [] }
    Echidnatron%
</code></pre>
<p>Great news!</p>
<h1><a class="header" href="#internals" id="internals">Internals</a></h1>
<h1><a class="header" href="#communication" id="communication">Communication</a></h1>
<p>Communication in timely dataflow starts from the <code>timely_communication</code> crate. This crate includes not only communication, but is actually where we start up the various worker threads and establish their identities. As in timely dataflow, everything starts by providing a per-worker closure, but this time we are given only a channel allocator as an argument.</p>
<p>Before continuing, I want to remind you that this is the <em>internals</em> section; you could write your code against this crate if you really want, but one of the nice features of timely dataflow is that you don't have to. You can use a nice higher level layer, as discussed previously in the document.</p>
<p>That being said, let's take a look at the example from the <code>timely_communication</code> documentation, which is not brief but shouldn't be wildly surprising either.</p>
<pre><code class="language-rust ignore">extern crate timely_communication;

fn main() {

    // extract the configuration from user-supplied arguments, initialize the computation.
    let config = timely_communication::Configuration::from_args(std::env::args()).unwrap();
    let guards = timely_communication::initialize(config, |mut allocator| {

        println!(&quot;worker {} of {} started&quot;, allocator.index(), allocator.peers());

        // allocates a pair of senders list and one receiver.
        let (mut senders, mut receiver) = allocator.allocate();

        // send typed data along each channel
        for i in 0 .. allocator.peers() {
            senders[i].send(format!(&quot;hello, {}&quot;, i));
            senders[i].done();
        }

        // no support for termination notification,
        // we have to count down ourselves.
        let mut received = 0;
        while received &lt; allocator.peers() {
            if let Some(message) = receiver.recv() {
                println!(&quot;worker {}: received: &lt;{}&gt;&quot;, allocator.index(), message);
                received += 1;
            }
        }

        allocator.index()
    });

    // computation runs until guards are joined or dropped.
    if let Ok(guards) = guards {
        for guard in guards.join() {
            println!(&quot;result: {:?}&quot;, guard);
        }
    }
    else { println!(&quot;error in computation&quot;); }
}
</code></pre>
<p>There are a few steps here, and we'll talk through the important parts in each of them.</p>
<h2><a class="header" href="#configuration" id="configuration">Configuration</a></h2>
<p>There is only a limited amount of configuration you can currently do in a timely dataflow computation, and it all lives in the <code>initialize::Configuration</code> type. This type is a simple enumeration of three ways a timely computation could run:</p>
<pre><code class="language-rust ignore">pub enum Configuration {
    Thread,
    Process(usize),
    Cluster(usize, usize, Vec&lt;String&gt;, bool)
}
</code></pre>
<p>The first variant <code>Thread</code> indicates that we will simply have one worker thread. This is a helpful thing to know because it means that all of our exchange channels can be dramatically simplified, just down to simple queues. The second variant <code>Process</code> corresponds to multiple worker threads within one process. The number indicates the parameters. The third variant <code>Cluster</code> is how we indicate that this process will participate in a larger clustered computation; we supply the number of threads, this process' identifier, a list of addresses of all participants, and a boolean for whether we would like some diagnostics about the established connections.</p>
<p>The configuration is important because it determines how we build the channel allocator <code>allocator</code> that we eventually provide to each worker: <code>allocator</code> will be responsible for building communication channels to other workers, and it will need to know where these other workers are.</p>
<h2><a class="header" href="#channel-allocators" id="channel-allocators">Channel Allocators</a></h2>
<p>The <code>allocator</code> reference bound by the worker closure is the only handle a worker has to the outside world (other than any values you move into the closure). It wraps up all the information we have about this workers place in the world, and provides the ability to assemble channels to the other workers.</p>
<p>There are a few implementations of the <code>Allocate</code> trait, which is defined as</p>
<pre><code class="language-rust ignore">pub trait Allocate {
    fn index(&amp;self) -&gt; usize;
    fn peers(&amp;self) -&gt; usize;
    fn allocate&lt;T: Data&gt;(&amp;mut self) -&gt; (Vec&lt;Box&lt;Push&lt;T&gt;&gt;&gt;, Box&lt;Pull&lt;T&gt;&gt;);
}
</code></pre>
<p>These methods are the only functionality provided by <code>allocator</code>. A worker can ask for its own index, which is a number from zero up to the number of total peer workers (including itself), which it can also ask for. Perhaps most importantly, the worker can also request the allocation of a typed channel, which is returned as a pair of (i) a list of <code>Push</code> endpoints into which it can send data, and (ii) a single <code>Pull</code> endpoint from which it can extract data. The list has length equal to the number of peers, and data sent into push endpoint <code>i</code> will eventually be received by the worker with index <code>i</code>, if it keeps pulling on its pull endpoint.</p>
<p>The channels are various and interesting, but should be smartly arranged. The channel from the worker back to itself is just a queue, the channels within the same process are Rust's inter-thread channels, and the channels between processes will automatically serialize and deserialize the type <code>T</code> for you (this is part of the <code>T: Data</code> requirement).</p>
<p>One crucial assumption made in this design is that the channels can be identified by their order of creation. If two workers start executing in different processes, allocating multiple channels, the only way we will know how to align these channels is by identifiers handed out as the channels are allocated. I strongly recommend against non-deterministic channel construction, or &quot;optimizing out&quot; some channels from some workers.</p>
<h3><a class="header" href="#the-data-trait-1" id="the-data-trait-1">The Data Trait</a></h3>
<p>The <code>Data</code> trait that we impose on all types that we exchange is a &quot;marker trait&quot;: it wraps several constraints together, like so</p>
<pre><code class="language-rust ignore">pub trait Data : Send+Any+Serialize+Clone+'static { }
impl&lt;T: Clone+Send+Any+Serialize+'static&gt; Data for T { }
</code></pre>
<p>These traits are all Rust traits, except for <code>Serialize</code>, and they mostly just say that we can clone and send the data around. The <code>Serialize</code> trait is something we introduce, and asks for methods to get into and out of a sequence of bytes.</p>
<pre><code class="language-rust ignore">pub trait Serialize {
    fn into_bytes(&amp;mut self, &amp;mut Vec&lt;u8&gt;);
    fn from_bytes(&amp;mut Vec&lt;u8&gt;) -&gt; Self;
}
</code></pre>
<p>We have a blanket implementation of <code>Serialize</code> for any type that implements <code>Abomonation</code>. Ideally, you shouldn't have to worry about this, unless you are introducing a new type and need an <code>Abomonation</code> implementation or you are hoping to move some types containing fields that do not satisfy those Rust traits.</p>
<h2><a class="header" href="#push-and-pull" id="push-and-pull">Push and Pull</a></h2>
<p>The two traits <code>Push</code> and <code>Pull</code> are the heart of the communication underlying timely dataflow. They are very simple, but relatively subtle and interesting and perhaps even under-exploited.</p>
<h3><a class="header" href="#push" id="push">Push</a></h3>
<p>The <code>Push</code> trait looks like so (with two helper methods elided):</p>
<pre><code class="language-rust ignore">pub trait Push&lt;T&gt; {
    fn push(&amp;mut self, element: &amp;mut Option&lt;T&gt;);
}
</code></pre>
<p>That's all of it.</p>
<p>The <code>push</code> method takes a mutable reference to an option wrapped around a thing. This is your way of telling the communication layer that, (i) if the reference points to a thing, you'd really like to push it into the channel, and (ii) if the reference doesn't point to a thing this is the cue that you might walk away for a while. It is important to send a <code>None</code> if you would like to ensure that whatever you've <code>push</code>ed in the past should be guaranteed to get through without further work on your part.</p>
<p>Now, we didn't need a mutable reference to do that; we could have just had the argument type be <code>Option&lt;T&gt;</code>, or had two methods <code>send</code> and <code>done</code> (those are the elided helper methods).</p>
<p>This framing allows for fairly natural and <em>stable</em> zero-copy communication. When you want to send a buffer of records, you wrap it up as <code>Some(buffer)</code> and call <code>push</code>. Once <code>push</code> returns, the channel has probably taken your buffer, but it has the opportunity to leave something behind for you. This is a very easy way for the communication infrastructure to <em>return</em> resources to you. In fact, even if you have finished sending messages, it may make sense to repeatedly send mutable references to <code>None</code> for as long as the channel has memory to hand you.</p>
<p>Although not used by timely at the moment, this is also designed to support zero copy networking where the communication layer below (e.g. something like RDMA) operates more efficiently if it allocates the buffers for you (e.g. in dedicated memory pinned by the hardware). In this case, <code>push</code> is a great way to <em>request</em> resources from the channel. Similarly, it can serve as a decent back-channel to return owned resources for the underlying typed data (e.g., you <code>push</code>ed a list of <code>String</code> elements, and once used they could be returned to you to be reused).</p>
<h3><a class="header" href="#pull" id="pull">Pull</a></h3>
<p>The <code>Pull</code> trait is the dual to <code>Push</code>: it allows someone on the other end of a channel to request whatever the channel has in store next, also as a mutable reference to an option wrapped around the type.</p>
<pre><code class="language-rust ignore">pub trait Pull&lt;T&gt; {
    fn pull(&amp;mut self) -&gt; &amp;mut Option&lt;T&gt;;
}
</code></pre>
<p>As before, the mutable reference and option allow the two participants to communicate about the availability of data, and to return resources if appropriate. For example, it is very natural after the call to <code>pull</code> to claim any <code>T</code> made available with a <code>::std::mem::swap</code> which puts something else in its place (either <code>Some(other)</code> or <code>None</code>). If the puller has some data to return, perhaps data it received from wherever it was pushing data at, this is a great opportunity to move it back up the communication chain.</p>
<p>I'm not aware of a circumstance where you might be obliged to call <code>pull</code> and set the result to <code>None</code> to signal that you may stop calling <code>Pull</code>. It seems like it could be important, if these methods really are dual, but I don't see how just yet.</p>
<h2><a class="header" href="#guarded-computation" id="guarded-computation">Guarded Computation</a></h2>
<p>The call to <code>initialize</code> returns a</p>
<pre><code class="language-rust ignore">Result&lt;WorkerGuards&lt;T&gt;,String&gt;
</code></pre>
<p>which is Rust's approach to error handling: we either get some worker guards or a <code>String</code> explaining why things went wrong, perhaps because we weren't able to establish connections with all of the workers, or something like that. The <code>WorkerGuards&lt;T&gt;</code> is a list of thread join handles, <code>::std::thread::JoinHandle&lt;T&gt;</code>, which is something that we can wait on and expect a <code>T</code> in return. Each of these handles allow us to wait on the local worker threads, and collect whatever they produce as output.</p>
<p>We've wrapped the handles up in a special type, <code>WorkerGuards</code>, because the default behavior otherwise should you just discard the result is for the threads to detach, which results in the <code>main</code> thread exiting and the workers just getting killed. This way, even if you ignore the result we will wait for the worker threads to complete. If you would like your main thread to exit and kill off the workers, you have other ways of doing this.</p>
<h1><a class="header" href="#progress-tracking" id="progress-tracking">Progress Tracking</a></h1>
<p>Progress tracking is a fundamental component of timely dataflow, and it is important to understand how it works to have a complete understanding of what timely dataflow does for you.</p>
<p>Let's start with a statement about what progress tracking means to accomplish.</p>
<p>The setting is that there are multiple workers, each of whom move data through a common dataflow graph. The data may move between workers, and as the data are processed by operators we have relatively few guarantees about their consequences: a worker may receive a record and do nothing, or it could send one thousand output records some of which are now destined for us. Nonetheless, we need to be able to make meaningful statements about the possibility of receiving more data.</p>
<p>Timely dataflow's approach is that data bear a logical <em>timestamp</em>, indicating some moment in the computation at which they should be thought to exist. This is not necessarily a physical timestamp, like the worker's clock when the record was created, but could be any type satisfying a few constraints. A common example is sequence numbers, counting up from zero.</p>
<p>Timely dataflow imposes a few constraints, we think they are natural, on the structure of the dataflow graph, from which it is able to make restrictive statements at each location in the dataflow graph of the form &quot;you will only ever see timestamps greater or equal to these times&quot;. This provides each dataflow operator with an understanding of <em>progress</em> in the computation. Eventually, we may even learn that the set of future timestamps is empty, indicating completion of the stream of data.</p>
<p>Timely dataflow computations are structured so that to send a timestamped message, an operator must hold a capability for that timestamp. Timely dataflow's progress tracking can be viewed as (i) workers collectively maintaining a view of outstanding timestamp capabilities at each location in the dataflow graph, and (ii) each worker independently determines and communicates the implications of changes in its view of capabilities to other locations in its instance of the dataflow graph.</p>
<p>Before we get into these two aspects, we will first need to be able to name parts of our dataflow graph.</p>
<h2><a class="header" href="#dataflow-structure" id="dataflow-structure">Dataflow Structure</a></h2>
<p>A dataflow graph hosts some number of operators. For progress tracking, these operators are simply identified by their index. Each operator has some number of <em>input ports</em>, and some number of <em>output ports</em>. The dataflow operators are connected by connecting each input port to a single output port (typically of another operator). Each output port may be connected to multiple distinct input ports (a message produced at an output port is to be delivered to all attached input ports).</p>
<p>In timely dataflow progress tracking, we identify output ports by the type <code>Source</code> and input ports by the type <code>Target</code>, as from the progress coordinator's point of view, an operator's output port is a <em>source</em> of timestamped data, and an operator's input port is a <em>target</em> of timestamped data. Each source and target can be described by their operator index and then an operator-local index of the corresponding port. The use of distinct types helps us avoid mistaking input and output ports.</p>
<pre><code class="language-rust ignore">pub struct Source {
    /// Index of the source operator.
    pub index: usize,
    /// Number of the output port from the operator.
    pub port: usize,
}

pub struct Target {
    /// Index of the target operator.
    pub index: usize,
    /// Number of the input port to the operator.
    pub port: usize,
}
</code></pre>
<p>The structure of the dataflow graph can be described by a list of all of the connections in the graph, a <code>Vec&lt;(Source, Target)&gt;</code>. From this, we could infer the number of operators and their numbers of input and output ports, as well as enumerate all of the connections themselves.</p>
<p>At this point we have the structure of a dataflow graph. We can draw a circle for each operator, a stub for each input and output port, and edges connecting the output ports to their destination input ports. Importantly, we have names for every location in the dataflow graph, which will either be a <code>Source</code> or a <code>Target</code>.</p>
<h2><a class="header" href="#maintaining-capabilities" id="maintaining-capabilities">Maintaining Capabilities</a></h2>
<p>Our first goal is for the workers to collectively track the number of outstanding timestamp capabilities in the system, for each timestamp and at each location, as dataflow operators run and messages are sent and received. Capabilities can exist in two places in timely dataflow: an operator can explicitly hold capabilities to send timestamped messages on each of its outputs, and each timestamped message bears a capability for its timestamp.</p>
<p>When tracking capabilities, we will track their <em>multiplicity</em>: how <em>many</em> capabilities for time <code>t</code> are there at location <code>l</code>? For most locations and times this number will be zero. Unless the computation has completed, for some locations and times this number must be positive. Numbers can also be transiently negative, as reports of changes may arrive out of order.</p>
<p>When a timely dataflow computation starts, there are no messages in flight. Rather, each operator starts with the capabilities to send any timestamped message on any of its outputs. As this is common knowledge among the workers, each initializes its counts with <code>#workers</code> for capabilities at each operator output. Each worker understands that it will need to hear <code>#workers</code> reports of such capabilities being dropped before they are actually out of the system.</p>
<p>As a computation proceeds, operators may perform three classes of action:</p>
<ol>
<li>They may consume input messages, acquiring the associated capability.</li>
<li>They may clone, downgrade, or drop any capability they hold.</li>
<li>They may send output messages at any timestamp for which they hold the capability.</li>
</ol>
<p>The results of these actions are a stream of changes to the occurrences of capabilities at each location in the dataflow graph. As input messages are consumed, capabilities located at the corresponding <code>Target</code> input port are decremented, and capabilities at the operators <code>Source</code> output ports are incremented. Cloning, downgrading, and dropping capabilities changes the counts at each of the operators corresponding <code>Source</code> output ports. Sending messages results in increments to the counts at the <code>Target</code> ports of each <code>Target</code> connected to the <code>Source</code> from which the message is sent.</p>
<p>Concretely, a batch of changes has the form <code>(Vec&lt;(Source, Time, i64)&gt;, Vec&lt;(Target, Time, i64)&gt;)</code>, indicating the increments and decrements for each time, at each dataflow location. Importantly we must keep these batches intact; the safety of the progress tracking protocol relies on not communicating half-formed progress messages (for example, consuming an input message but forgetting to indicate the acquisition of its capability).</p>
<p>Each worker broadcasts the stream of progress change batches to all workers in the system (including itself) along point-to-point FIFO channels. At any point in time, each worker has seen an arbitrary prefix of the sequence of progress change batches produced by each other worker, and it is expected that as time proceeds each worker eventually sees every prefix of each sequence.</p>
<p>At any point in time, each worker's view of the capabilities in the system is defined by the accumulation of all received progress change batches, plus initial capabilities at each output (with starting multiplicity equal to the number of workers). This view may be surprising and messy (there may be negative counts), but at all times it satisfies an important safety condition, related to the communicated implications of these capabilities, which we now develop.</p>
<h2><a class="header" href="#communicating-implications" id="communicating-implications">Communicating Implications</a></h2>
<p>Each worker maintains an accumulation of progress update batches, which explains where capabilities may exist in the dataflow graph. This information is useful, but it is not yet sufficient to make strong statements about the possibility of timestamped messages arriving at <em>other</em> locations in the dataflow graph. Even though a capability for timestamp <code>t</code> may exist, this does not mean that the time may arrive at any location in the dataflow graph. To be precise, we must discuss the paths through the dataflow graph which the timestamp capability could follow.</p>
<h3><a class="header" href="#path-summaries" id="path-summaries">Path Summaries</a></h3>
<p>Progress tracking occurs in the context of a dataflow graph of operators all with a common timestamp type <code>T: Timestamp</code>. The <code>Timestamp</code> trait requires the <code>PartialOrder</code> trait, meaning two timestamps may be ordered but need not be. Each type implementing <code>Timestamp</code> must also specify an associated type <code>Summary</code> implementing <code>PathSummary&lt;Self&gt;</code>.</p>
<pre><code class="language-rust ignore">pub trait Timestamp: PartialOrder {
    type Summary : PathSummary&lt;Self&gt;;
}
</code></pre>
<p>A path summary is informally meant to summarize what <em>must</em> happen to a timestamp as it travels along a path in a timely dataflow. Most paths that you might draw have trivial summaries (&quot;no change guaranteed&quot;), but some paths do force changes on timestamps. For example, a path that goes from the end of a loop back to its head <em>must</em> increment the loop counter coordinate of any timestamp capability passed along it.</p>
<pre><code class="language-rust ignore">pub trait PathSummary&lt;T&gt; : PartialOrder {
    fn results_in(&amp;self, src: &amp;T) -&gt; Option&lt;T&gt;;
    fn followed_by(&amp;self, other: &amp;Self) -&gt; Option&lt;Self&gt;;
}
</code></pre>
<p>The types implementing <code>PathSummary</code> must be partially ordered, and implement two methods:</p>
<ol>
<li>The <code>results_in</code> method explains what must happen to a timestamp moving along a path. Note the possibility of <code>None</code>; a timestamp could <em>not</em> move along a path. For example, a path summary <code>path</code> could increment a timestamp by one, for which</li>
</ol>
<pre><code class="language-rust ignore">path.results_in(&amp;4) == Some(5);
path.results_in(&amp;u64::max_value()) == None;
</code></pre>
<p>It is important that <code>results_in</code> only advance timestamps: for all path summaries <code>p</code> we require that</p>
<pre><code class="language-rust ignore">if let Some(time) = p.results_in(&amp;x) {
    assert!(x.less_equal(&amp;time));
}
</code></pre>
<ol start="2">
<li>The <code>followed_by</code> method explains how two path summaries combine. When we build the summaries we will start with paths corresponding to single edges, and build out more complex paths by combining the effects of multiple paths (and their summaries). As with <code>results_in</code>, it may be that two paths combined result in something that will never pass a timestamp, and the summary may be <code>None</code> (for example, two paths that each increment a loop counter by half of its maximum value).</li>
</ol>
<p>Two path summaries are ordered if for all timestamps the two results of the path summaries applied to the timestamp are also ordered.</p>
<p>Path summaries are only partially ordered, and when summarizing what must happen to a timestamp when going from one location to another, along one of many paths, we will quickly find ourselves speaking about <em>collections</em> of path summaries. There may be several summaries corresponding to different paths we might take. We can discard summaries from this collection that are strictly greater than other elements of the collection, but we may still have multiple incomparable path summaries.</p>
<p>The second part of progress tracking, communicating the implications of current capabilities, is fundamentally about determining and locking in the minimal collections of path summaries between any two locations (<code>Source</code> or <code>Target</code>) in the dataflow graph. This is the &quot;compiled&quot; representation of the dataflow graph, from which we can derive statements about the possibility of messages at one point in the graph leading to messages at another point.</p>
<h3><a class="header" href="#operator-summaries" id="operator-summaries">Operator summaries</a></h3>
<p>Where do path summaries come from?</p>
<p>Each operator in timely dataflow must implement the <code>Operate</code> trait. Among other things, that we will get to, the <code>Operate</code> trait requires that the operator summarize <em>itself</em>, providing a collection of path summaries for each of its internal paths, from each of its inputs to each of its outputs. The operator must describe what timestamps could possibly result at each of its output as a function of a timestamped message arriving at each of its inputs.</p>
<p>For most operators, this summary is simply: &quot;timestamped data at each of my inputs could result in equivalently timestamped data at each of my outputs&quot;. This is a fairly simple summary, and while it isn't very helpful in making progress, it is the only guarantee the operator can provide.</p>
<p>The <code>Feedback</code> operator is where most of our interesting path summaries start: it is the operator found in feedback loops that ensures that a specific coordinate of the timestamp is incremented. All cycles in a timely dataflow graph must pass through such an operator, and in cyclic dataflow we see non-trivial summaries from the outputs of downstream operators to the inputs of upstream operators.</p>
<p>Another useful summary is the absence of a summary. Sometimes there is no path between two points in a dataflow graph. Perhaps you do some computation on your dataflow inputs, and then introduce them into an iterative computation; there is no path that returns that data to the computation you do on the inputs. Records in-flight in the iterative subcomputation will not block the computation you do on your inputs, because we know that there is no path back from the iteration to the inputs.</p>
<p>For example, an operator could plausibly have two inputs, a data input and a diagnostic input, and corresponding data and diagnostic outputs. The operator's internal summary could reveal that diagnostic input cannot result in data output, which allows us to issue diagnostic queries (as data for that input) without blocking downstream consumers of the data output. Timely dataflow can see that even though there are messages in flight, they cannot reach the data output and need not be on the critical path of data computation.</p>
<h3><a class="header" href="#a-compiled-representation" id="a-compiled-representation">A Compiled Representation</a></h3>
<p>From the operator summaries we build path summaries, and from the path summaries we determine, for every pair of either <code>Source</code> or <code>Target</code> a collection of path summaries between the two. How could a timestamped message at one location lead to timestamped messages at the other?</p>
<p>The only constraint we require is that there should be no cycles that do not strictly advance a timestamp.</p>
<h2><a class="header" href="#a-safety-property" id="a-safety-property">A Safety Property</a></h2>
<p>Each operator maintains a collection of counts of timestamp capabilities at each location (<code>Source</code> or <code>Target</code>) in the dataflow graph. At the same time, there is a statically defined set of path summaries from each location to any other location in the dataflow graph.</p>
<p>The safety property is: for any collection of counts resulting from the accumulation of arbitrary prefixes of progress update batches from participating workers, if for any location <code>l1</code> in the dataflow graph and timestamp <code>t1</code> there is not another location <code>l2</code> and timestamp <code>t2</code> with strictly positive accumulated count such that there is a path summary <code>p</code> from <code>l2</code> to <code>l1</code> with <code>p(t2) &lt;= t1</code>, then no message will ever arrive at location <code>l1</code> bearing timestamp <code>t1</code>.</p>
<p>This property has <a href="https://www.microsoft.com/en-us/research/publication/the-naiad-clock-protocol-specification-model-checking-and-correctness-proof/">a formalization in TLA</a> courtesy of Tom Rodeheffer, but let's try to develop the intuition for its correctness.</p>
<p>Actually, let's first develop some <em>counter</em>-intuition for its correctness. This property holds, but we've said almost nothing about the communication of messages between workers. The only property we use about message delivery in proving the safety property is that it is &quot;at most once&quot;; messages should not be multiplied in flight. Does it matter in which order messages are delivered? No. Does it matter that messages are <em>ever</em> delivered? No (this is only a safety property). Can operators consume, digest, and send messages that their local progress protocol doesn't even know exist yet? Yes.</p>
<p>There is almost no coordination between the data plane, on which messages get sent, and the control plane, along which progress update batches get sent. The only requirement is that you do not send a progress update batch for some action that you have not performed.</p>
<p>So where do we find intuition for the correctness of the protocol?</p>
<p>Although we may have only seen prefixes of the progress update batches from other workers, we can nonetheless reason about what future progress update batches from each worker will need to look like. In the main, we will use the property that if updates correspond to things that actually happen:</p>
<ol>
<li>Any message consumed must have a corresponding message produced, even if we haven't heard about it yet.</li>
<li>Any message produced must involve a capability held, even if we haven't heard about it yet.</li>
<li>Any capability held must involve either another capability held, or a message consumed.</li>
</ol>
<p>We'll argue that for any sequence of progress updates some prefix of which accumulates as assumed in the statement of the safety property, then no extension of these prefixes could possibly include an update acknowledging a message received at <code>l1</code> with timestamp <code>t1</code>.</p>
<p>First, we define an order on the pairs <code>(li, ti)</code> of locations and timestamps, in the Naiad paper called <em>pointstamps</em>. Pointstamps are partially ordered by the <em>could-result-in</em> relation: <code>(li, ti)</code> <em>could-result-in</em> <code>(lj, tj)</code> if there is a path summary <code>p</code> from <code>li</code> to <code>lj</code> where <code>p(ti) &lt;= tj</code>. This is an order because it is (i) reflexive by definition, (ii) antisymmetric because two distinct pointstamps that <em>could-result-in</em> each other would imply a cycle that does not strictly advance the timestamp, and (iii) transitive by the assumed correctness of path summary construction.</p>
<p>Next, each progress update batch has a property, one that is easier to state if we imagine operators cannot clone capabilities, and must consume a capability to send a message: each atomic progress update batch decrements a pointstamp and optionally increments pointstamps strictly greater than it. Alternately, each progress update batch that increments a pointstamp must decrement a pointstamp strictly less than it. Inductively, any collection of progress update batches whose net effect increments a pointstamp must have a net effect decrementing some pointstamp strictly less than it.</p>
<p>Now, just before a worker might hypothetically receive a message at <code>(l1, t1)</code>, we freeze the system. We stop performing new actions, and instead resolve all progress updates for actions actually performed. Let these progress update batches circulate, so that the worker in question now has a complete and current view of the frozen state of the system. All pointstamp counts should now be non-negative.</p>
<p>The additional progress updated batches received as part of stabilizing the system can only have a net positive effect on <code>(l1, t1)</code> if they have a net negative effect on some poinstamp strictly less than it. However, as the final accumulation is non-negative, this can only happen if the strictly prior pointstamp had a positive accumulation before stabilization. If no such pointstamp has a positive accumulation before stabilization, it is not possible for <code>(l1, t1)</code> to have a positive accumulation at stabilization, and consequently there cannot be a message waiting to be received.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
